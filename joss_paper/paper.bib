@article{le0,
	title = {Higher order dynamic mode decomposition of noisy experimental data: {The} flow structure of a zero-net-mass-flux jet},
	volume = {88},
	issn = {08941777},
	shorttitle = {Higher order dynamic mode decomposition of noisy experimental data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089417771730184X},
	doi = {10.1016/j.expthermflusci.2017.06.011},
	language = {en},
	urldate = {2019-03-06},
	journal = {Experimental Thermal and Fluid Science},
	author = {Le Clainche, Soledad and Vega, Jos{\'e} M. and Soria, Julio},
	month = nov,
	year = {2017},
	pages = {336--353},
}

@article{giannakis1,
	title = {Data-driven spectral decomposition and forecasting of ergodic dynamical systems},
	volume = {47},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520317300982},
	doi = {10.1016/j.acha.2017.09.001},
	language = {en},
	number = {2},
	urldate = {2019-12-14},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Giannakis, Dimitrios},
	month = sep,
	year = {2019},
	pages = {338--396},
}

@article{champion2,
	title = {Discovery of {Nonlinear} {Multiscale} {Systems}: {Sampling} {Strategies} and {Embeddings}},
	volume = {18},
	issn = {1536-0040},
	shorttitle = {Discovery of {Nonlinear} {Multiscale} {Systems}},
	url = {https://epubs.siam.org/doi/10.1137/18M1188227},
	doi = {10.1137/18M1188227},
	language = {en},
	number = {1},
	urldate = {2019-03-06},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Champion, Kathleen P. and Brunton, Steven L. and Kutz, J. Nathan},
	month = jan,
	year = {2019},
	pages = {312--333},
}

@article{tu3,
	title = {On {Dynamic} {Mode} {Decomposition}: {Theory} and {Applications}},
	volume = {1},
	issn = {2158-2491},
	shorttitle = {On {Dynamic} {Mode} {Decomposition}},
	url = {http://arxiv.org/abs/1312.0041},
	doi = {10.3934/jcd.2014.1.391},
	language = {en},
	number = {2},
	urldate = {2019-03-06},
	journal = {Journal of Computational Dynamics},
	author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
	month = dec,
	year = {2014},
	note = {arXiv: 1312.0041},
	keywords = {koopman},
	pages = {391--421},
}

@article{williams4,
	title = {A {Data}{\textendash}{Driven} {Approximation} of the {Koopman} {Operator}: {Extending} {Dynamic} {Mode} {Decomposition}},
	volume = {25},
	issn = {0938-8974, 1432-1467},
	shorttitle = {A {Data}{\textendash}{Driven} {Approximation} of the {Koopman} {Operator}},
	url = {http://link.springer.com/10.1007/s00332-015-9258-5},
	doi = {10.1007/s00332-015-9258-5},
	language = {en},
	number = {6},
	urldate = {2019-03-13},
	journal = {Journal of Nonlinear Science},
	author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
	month = dec,
	year = {2015},
	pages = {1307--1346},
}

@inproceedings{rabin6,
	title = {Heterogeneous datasets representation and learning using diffusion maps and {Laplacian} pyramids},
	isbn = {978-1-61197-232-0 978-1-61197-282-5},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972825.17},
	doi = {10.1137/1.9781611972825.17},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {Proceedings of the 2012 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Rabin, Neta and Coifman, Ronald R.},
	month = apr,
	year = {2012},
	pages = {189--199},
}

@article{coifman7,
	title = {Geometric harmonics: {A} novel tool for multiscale out-of-sample extension of empirical functions},
	volume = {21},
	issn = {10635203},
	shorttitle = {Geometric harmonics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000522},
	doi = {10.1016/j.acha.2005.07.005},
	language = {en},
	number = {1},
	urldate = {2019-12-15},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
	month = jul,
	year = {2006},
	pages = {31--52},
}

@article{coifman8,
	title = {Diffusion maps},
	volume = {21},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
	doi = {10.1016/j.acha.2006.04.006},
	language = {en},
	number = {1},
	urldate = {2019-12-15},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
	month = jul,
	year = {2006},
	pages = {5--30},
}

@article{bengio9,
	title = {Learning {Eigenfunctions} {Links} {Spectral} {Embedding} and {Kernel} {PCA}},
	volume = {16},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/0899766041732396},
	doi = {10.1162/0899766041732396},
	language = {en},
	number = {10},
	urldate = {2019-12-15},
	journal = {Neural Computation},
	author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le and Paiement, Jean-Fran{\c c}ois and Vincent, Pascal and Ouimet, Marie},
	month = oct,
	year = {2004},
	pages = {2197--2219},
}

@article{fernandez10,
	title = {Auto-adaptive multi-scale {Laplacian} {Pyramids} for modeling non-uniform data},
	volume = {93},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197620301202},
	doi = {10.1016/j.engappai.2020.103682},
	language = {en},
	urldate = {2020-07-13},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Fern{\'a}ndez, {\'A}ngela and Rabin, Neta and Fishelov, Dalia and Dorronsoro, Jos{\'e} R.},
	month = aug,
	year = {2020},
	pages = {103682},
}

@book{kutz12,
	title = {Dynamic mode decomposition. {Data}-{Driven} modelling of complex systems},
	isbn = {978-1-61197-450-8},
	language = {en},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kutz, J. Nathan and Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L.},
	year = {2016},
}

@article{schmid13,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112010001217/type/journal_article},
	doi = {10.1017/S0022112010001217},
	language = {en},
	urldate = {2020-04-09},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	month = aug,
	year = {2010},
	pages = {5--28},
}

@article{belkin14,
	title = {Laplacian {Eigenmaps} for {Dimensionality} {Reduction} and {Data} {Representation}},
	volume = {15},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976603321780317},
	doi = {10.1162/089976603321780317},
	language = {en},
	number = {6},
	urldate = {2020-04-29},
	journal = {Neural Computation},
	author = {Belkin, Mikhail and Niyogi, Partha},
	month = jun,
	year = {2003},
	pages = {1373--1396},
}

@article{donoho15,
	title = {Hessian eigenmaps: {Locally} linear embedding techniques for high-dimensional data},
	volume = {100},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Hessian eigenmaps},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1031596100},
	doi = {10.1073/pnas.1031596100},
	language = {en},
	number = {10},
	urldate = {2020-04-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Donoho, D. L. and Grimes, C.},
	month = may,
	year = {2003},
	pages = {5591--5596},
}

@book{bishop16,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@article{pedregosa17,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	language = {en},
	journal = {Machine Learning in Python},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	year = {2011},
	pages = {6},
}

@article{brunton18,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1517384113},
	doi = {10.1073/pnas.1517384113},
	language = {en},
	number = {15},
	urldate = {2020-04-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pages = {3932--3937},
}

@article{chiavazzo19,
	title = {Reduced {Models} in {Chemical} {Kinetics} via {Nonlinear} {Data}-{Mining}},
	volume = {2},
	issn = {2227-9717},
	url = {http://www.mdpi.com/2227-9717/2/1/112},
	doi = {10.3390/pr2010112},
	language = {en},
	number = {1},
	urldate = {2019-12-14},
	journal = {Processes},
	author = {Chiavazzo, Eliodoro and Gear, Charles and Dsilva, Carmeline and Rabin, Neta and Kevrekidis, Ioannis},
	month = jan,
	year = {2014},
	pages = {112--140},
}

@article{desilva20,
	title = {{PySINDy}: {A} {Python} package for the sparse identification of nonlinear dynamical systems from data},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {{PySINDy}},
	url = {https://joss.theoj.org/papers/10.21105/joss.02104},
	doi = {10.21105/joss.02104},
	abstract = {Scientists have long quantified empirical observations by developing mathematical models that characterize the observations, have some measure of interpretability, and are capable of making predictions. Dynamical systems models in particular have been widely used to study, explain, and predict system behavior in a wide range of application areas, with examples ranging from Newton{\textquoteright}s laws of classical mechanics to the Michaelis-Menten kinetics for modeling enzyme kinetics. While governing laws and equations were traditionally derived by hand, the current growth of available measurement data and resulting emphasis on data-driven modeling motivates algorithmic approaches for model discovery. A number of such approaches have been developed in recent years and have generated widespread interest, including Eureqa (Schmidt \& Lipson, 2009), sure independence screening and sparsifying operator (Ouyang, Curtarolo, Ahmetcik, Scheffler, \& Ghiringhelli, 2018), and the sparse identification of nonlinear dynamics (SINDy) (Brunton, Proctor, \& Kutz, 2016). Maximizing the impact of these model discovery methods requires tools to make them widely accessible to scientists across domains and at various levels of mathematical expertise.},
	language = {en},
	number = {49},
	urldate = {2020-06-15},
	journal = {Journal of Open Source Software},
	author = {de Silva, Brian and Champion, Kathleen and Quade, Markus and Loiseau, Jean-Christophe and Kutz, J. and Brunton, Steven},
	month = may,
	year = {2020},
	pages = {2104},
}

@incollection{paszke21,
title = {{PyTorch}: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8026--8037},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{seabold22,
	address = {Austin, Texas},
	title = {Statsmodels: {Econometric} and {Statistical} {Modeling} with {Python}},
	shorttitle = {Statsmodels},
	url = {https://conference.scipy.org/proceedings/scipy2010/seabold.html},
	doi = {10.25080/Majora-92bf1922-011},
	abstract = {Statsmodels is a library for statistical and econometric analysis in Python. This paper discusses the current relationship between statistics and Python and open source more generally, outlining how the statsmodels package fills a gap in this relationship. An overview of statsmodels is provided, including a discussion of the overarching design and philosophy, what can be found in the package, and some usage examples. The paper concludes with a look at what the future holds.},
	language = {en},
	urldate = {2020-06-15},
	author = {Seabold, Skipper and Perktold, Josef},
	year = {2010},
	pages = {92--96},
}

@article{demo23,
	title = {{PyDMD}: {Python} {Dynamic} {Mode} {Decomposition}},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {{PyDMD}},
	url = {http://joss.theoj.org/papers/10.21105/joss.00530},
	doi = {10.21105/joss.00530},
	abstract = {Dynamic mode decomposition (DMD) is a model reduction algorithm developed by Schmid (Schmid 2010). Since then has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. It is used for a data-driven model simplification based on spatiotemporal coherent structures. DMD relies only on the high-fidelity measurements, like experimental data and numerical simulations, so it is an equation-free algorithm. Its popularity is also due to the fact that it does not make any assumptions about the underlying system. See (Kutz et al. 2016) for a comprehensive overview of the algorithm and its connections to the Koopman-operator analysis, initiated in (Koopman 1931), along with examples in computational fluid dynamics. In the last years many variants arose, such as multiresolution DMD, compressed DMD, forward backward DMD, and higher order DMD among others, in order to deal with noisy data, big dataset, or spurius data for example. In the PyDMD package ({\textquotedblleft}PyDMD: Python Dynamic Mode Decomposition. Available at: https://github.com/mathLab/PyDMD{\textquotedblright} n.d.) we implemented in Python the majority of the variants mentioned above with a user friendly interface. We also provide many tutorials that show all the characteristics of the software, ranging from the basic use case to the most sofisticated one allowed by the package.},
	language = {en},
	number = {22},
	urldate = {2020-06-15},
	journal = {The Journal of Open Source Software},
	author = {Demo, Nicola and Tezzele, Marco and Rozza, Gianluigi},
	month = feb,
	year = {2018},
	pages = {530},
}

@misc{abadi24,
  author   = {Abadi, Mart{\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  url={https://www.tensorflow.org/},
  note={Software available from tensorflow.org},
  title    = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Distributed} {Systems}},
  year     = {2015},
  pages    = {19},
  language = {en},
}

@InProceedings{abadi26,
  author    = {Abadi, Mart\'{\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  booktitle = {Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation},
  title     = {{TensorFlow}: {A} {System} for {Large}-{Scale} {Machine} {Learning}},
  year      = {2016},
  address   = {USA},
  pages     = {265–283},
  publisher = {USENIX Association},
  series    = {OSDI’16},
  isbn      = {9781931971331},
  location  = {Savannah, GA, USA},
  numpages  = {19},
}

@article{hochreiter25,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insu cient, decaying error back ow. We brie y review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, e cient, gradient-based method called {\textbackslash}Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through {\textbackslash}constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with arti cial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, arti cial long time lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2020-06-17},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
}
