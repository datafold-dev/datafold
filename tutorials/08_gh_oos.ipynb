{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Harmonics: interpolate function values on data manifold\n",
    "\n",
    "Geometric harmonics are eigenvectors (corresponding to point evaluations of eigenfunctions) of a kernel matrix computed on data. For example, the kernel matrix from a Gaussian kernel\n",
    "\n",
    "$$W_{i,j} = \\exp{(-d^2_{i,j}/\\varepsilon)}, i,j = 1,\\ldots,M$$\n",
    "\n",
    "have geometric harmonics $\\phi$ and corresponding eigenvalues $\\lambda$ from, $(\\{\\phi_i\\}_{i=1}^{M}, \\{\\lambda\\}_{i=1}^{M} = \\operatorname{eig}(W)$. Performing interpolation with geometric harmonics builds up on the idea of the Nyström extension: instead of extending the geometric harmonics (i.e. eigenvectors) itself to a neighborhood region, the method allows to interpolate arbitrary function values defined on a manifold. \n",
    "\n",
    "The method is described in detail in\n",
    "\n",
    "*Coifman, Ronald R., and Stéphane Lafon. “Geometric Harmonics: A Novel Tool for Multiscale out-of-Sample Extension of Empirical Functions.” Applied and Computational Harmonic Analysis 21, no. 1 (July 2006): 31–52. https://doi.org/10.1016/j.acha.2005.07.005.*\n",
    "\n",
    "**Usecase: an out-of-sample method for Diffusion Map model**\n",
    "\n",
    "We set up a Diffusion Maps model that parametrizes an intrinsic low dimensional manifold $\\Psi$ in a high-dimensional dataset $X$. After we embedded the available data, we want to now learn the mapping\n",
    "\n",
    "$$f(X): X \\rightarrow \\Psi$$\n",
    "\n",
    "also for new samples $x_{new} \\notin X$ (image) or $\\psi \\notin \\Psi$ (pre-image). This is referred to as the so-called \"out-of-sample extension\". For image mappings, an out-of-sample method must be able to handle the high-dimensional ambient space of $ X $.\n",
    "\n",
    "The `DiffusionMaps` model maps the three-dimensional swiss-roll data $X$ to the two-dimensional manifold embedding $\\Psi$. We then learn an out-of-sample mapping $f$ and pre-image $f^{-1}$ between the two spaces with a geometric harmonics interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3  # noqa: F401\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "from datafold import GeometricHarmonicsInterpolator as GHI\n",
    "from datafold import LaplacianPyramidsInterpolator, LocalRegressionSelection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold embedding of swiss-roll data with Diffusion Maps\n",
    "\n",
    "We first generate the swiss-roll dataset with scikit-learn and then fit a `DiffusionMaps` model to compute the manifold embedding on the generated data. In this tutorial we skip the eigenvector selection process and choose the suitable embedding $\\Psi = \\{\\psi_1, \\psi_5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain dataset with a lot of points to get accurate eigenfunctions\n",
    "rng = np.random.default_rng(1)\n",
    "X_all, color = make_swiss_roll(n_samples=15000, noise=0, random_state=1)\n",
    "\n",
    "num_eigenpairs = 6\n",
    "\n",
    "pcm = pfold.PCManifold(X_all)\n",
    "pcm.optimize_parameters(random_state=0)\n",
    "\n",
    "dmap = dfold.DiffusionMaps(\n",
    "    pfold.GaussianKernel(\n",
    "        epsilon=pcm.kernel.epsilon, distance=dict(cut_off=pcm.cut_off)\n",
    "    ),\n",
    "    n_eigenpairs=num_eigenpairs,\n",
    ")\n",
    "dmap = dmap.fit(pcm)\n",
    "evecs, evals = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "\n",
    "# find the best eigenvectors automatically\n",
    "selection = LocalRegressionSelection(\n",
    "    intrinsic_dim=2, n_subsample=500, strategy=\"dim\"\n",
    ").fit(dmap.eigenvectors_)\n",
    "\n",
    "psi_all = evecs[:, selection.evec_indices_]\n",
    "\n",
    "# select a subset of the data to proceed, so that the next computations are less expensive\n",
    "ind_subset = np.sort(rng.permutation(np.arange(X_all.shape[0]))[0:2500])\n",
    "X_all = X_all[ind_subset, :]\n",
    "psi_all = psi_all[ind_subset, :]\n",
    "color = color[ind_subset]\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "fig.suptitle(f\"total #samples={pcm.shape[0]}\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "ax.set_title(\"all data, original coordinates\")\n",
    "ax.scatter(*X_all.T, s=5, c=color, cmap=plt.cm.Spectral)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"all data, embedding coordinates\")\n",
    "ax.scatter(*psi_all.T, s=5, c=color, cmap=plt.cm.Spectral)\n",
    "ax.set_xlabel(r\"$\\psi_\" + str(selection.evec_indices_[0]) + r\"$\")\n",
    "ax.set_ylabel(r\"$\\psi_\" + str(selection.evec_indices_[1]) + r\"$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image mapping\n",
    "\n",
    "We now have both the original dataset $X$ (=`X_all`) and the embedding $\\Psi$ (=`psi_all`). We proceed to train the `GeometricHarmonicsInterpolator` model, which will perform the out-of-sample mapping. For the image mapping, we want to learn the function $f: X \\rightarrow \\Psi$. Here, the ambient space of $X$ is relatively low with only three dimensions, which is mainly for plotting purposes. If the same data manifold was embedded in a much larger space, say 100 by linearly transforming it into this space, we would obtain the same results.\n",
    "\n",
    "We split the datasets $X$ and $\\Psi$ into a training and test set accordingly. We do not use the test set for parameter optimization, but later to compare the out-of-sample for \"new\" points against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the psi_test values are used for the \"ground truth\" values to measure an error\n",
    "# the color values are used for plotting\n",
    "X_train, X_test, psi_train, psi_test, color_train, color_test = train_test_split(\n",
    "    X_all, psi_all, color, train_size=2 / 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GeometricHarmonicsInterpolator` has two important parameters, the number of geometric harmonics (`n_eigenpairs`) and the kernel scale (`epsilon`). We manually select the two values and refer to the end of this tutorial where we also use Bayesian optimization to find suitable parameters in the pre-image problem. \n",
    "\n",
    "Note that the `DiffusionMaps` model also comes with a native image mapping, based on the Nyström extension. A direct comparison, however, is difficult, because here we use the `DiffusionMap` embedding as ground truth and cannot map truly new samples. \n",
    "\n",
    "**Note**: The eigenvalues corresponding to the geometric harmonics (kernel eigenvectors) are $\\lambda_i \\rightarrow 0, \\text{ for } i \\rightarrow \\infty$. The geometric harmonics interpolation (like the Nyström extension) involves the reciprocal of eigenvalues $1/\\lambda_i$. This means, that too many eigenpairs can lead to numerical instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the geometric harmonics from X to \\Psi\n",
    "n_eigenpairs = 100\n",
    "epsilon = 30\n",
    "\n",
    "# construct the GeometricHarmonicsInterpolator and fit it to the data.\n",
    "gh_interpolant = GHI(\n",
    "    pfold.GaussianKernel(epsilon=epsilon),\n",
    "    n_eigenpairs=n_eigenpairs,\n",
    ")\n",
    "\n",
    "gh_interpolant.fit(X_train, psi_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate interpolation function for image\n",
    "\n",
    "Because `GeometricHarmonicsInterpolator` follows the scikit-learn API, we can now use the `.score` method to evaluate the residual and the error for the validation points. The default metric is a mean squared error. \n",
    "\n",
    "**Note** that the model scores are the mean squared error, we therefore maximize negative errors. This is according to the definition of scikit-learn's scoring parameter which [states](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) \"higher return values are better than lower return values\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train = gh_interpolant.score(X_train, psi_train)\n",
    "error_test = gh_interpolant.score(X_test, psi_test)\n",
    "\n",
    "# use pandas for table\n",
    "df = pd.DataFrame(\n",
    "    np.row_stack([residual_train, error_test]),\n",
    "    index=[\"residual\", \"error\"],\n",
    "    columns=[\"psi1\", \"psi5\"],\n",
    ")\n",
    "\n",
    "print(f\"mean: \\n{df.mean()}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left side, we plot the ground truth test set embedding (from the `DiffusionMaps` model). On the right side, we predict the embedding values with geometric harmonics interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[12, 5], sharey=True)\n",
    "\n",
    "ax[0].set_title(\"ground truth test set ${\\psi}_{1,5}$\")\n",
    "ax[0].scatter(psi_test[:, 0], psi_test[:, 1], s=10, c=color_test, cmap=plt.cm.Spectral)\n",
    "ax[0].set_xlabel(r\"$\\psi_1$\")\n",
    "ax[0].set_ylabel(r\"$\\psi_5$\")\n",
    "\n",
    "ax[1].scatter(\n",
    "    *gh_interpolant.predict(X_test).T, s=10, c=color_test, cmap=plt.cm.Spectral\n",
    ")\n",
    "ax[1].set_title(r\"interpolated values $\\hat{\\psi}_{1,5}$\")\n",
    "ax[1].set_xlabel(r\"$\\hat{\\psi}_1$\")\n",
    "ax[1].set_ylabel(r\"$\\hat{\\psi}_5$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_test_interp = gh_interpolant.predict(X_test)\n",
    "\n",
    "error_color = (psi_test[:, 0] - psi_test_interp[:, 0]) ** 2 + (\n",
    "    psi_test[:, 1] - psi_test_interp[:, 1]\n",
    ") ** 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[12, 5], sharey=True)\n",
    "\n",
    "sc = ax[0].scatter(psi_test[:, 0], psi_test[:, 1], c=error_color, cmap=\"Reds\")\n",
    "ax[0].set_title(\"test data with error (absolute)\")\n",
    "plt.colorbar(sc, ax=ax[0])\n",
    "ax[0].set_xlabel(r\"${\\psi}_1$\")\n",
    "ax[0].set_ylabel(r\"${\\psi}_5$\")\n",
    "\n",
    "# the np.newaxis need are required to have 2D arrays:\n",
    "norm_factor = np.max(\n",
    "    [\n",
    "        np.max(pdist(psi_test[0, :][:, np.newaxis])),\n",
    "        np.max(pdist(psi_test[1, :][:, np.newaxis])),\n",
    "    ]\n",
    ")  # take max. distance in test dataset as the norming factor\n",
    "\n",
    "sc = ax[1].scatter(\n",
    "    psi_test[:, 0],\n",
    "    psi_test[:, 1],\n",
    "    vmin=0,\n",
    "    vmax=0.1,\n",
    "    c=error_color / norm_factor,\n",
    "    cmap=\"Reds\",\n",
    ")\n",
    "plt.colorbar(sc, ax=ax[1])\n",
    "ax[1].set_title(\"test data with error (relative)\")\n",
    "ax[1].set_xlabel(r\"${\\psi}_1$\")\n",
    "ax[1].set_ylabel(r\"${\\psi}_5$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Image mapping\n",
    "\n",
    "We now want to learn the inverse mapping and interpolate the $X$ values (as function values) on $\\Psi$.\n",
    "\n",
    "$$f^{-1}: \\Psi \\rightarrow X$$\n",
    " \n",
    "In the literature, this is often referred to as the pre-image problem of manifold learning.\n",
    "\n",
    "We repeat the above steps and use the same embedding (but re-shuffle the training and test set). \n",
    "\n",
    "In the following, we will train a `LaplacianPyramidsInterpolator` model, for details see\n",
    "\n",
    "Rabin, Neta, and Ronald R. Coifman. \"Heterogeneous datasets representation and learning using diffusion maps and Laplacian pyramids.\" Proceedings of the 2012 SIAM International Conference on Data Mining. Society for Industrial and Applied Mathematics, 2012. https://doi.org/10.1137/1.9781611972825.17 \n",
    "\n",
    "The model is generalized to handle multiple target values (here three). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpi = LaplacianPyramidsInterpolator(residual_tol=0.001)\n",
    "lpi.fit(psi_train, X_train)\n",
    "lpi.score(psi_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare ground truth points and points reconstructed in pre-image map in plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth and interpolated values\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"ground truth values\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "ax.scatter(*(lpi.predict(psi_test)).T, c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"interpolated values\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare error of reconstruction in error plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot error\n",
    "error_color = np.linalg.norm(X_test - lpi.predict(psi_test), axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "sc = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=error_color, cmap=\"Reds\")\n",
    "plt.title(\"test data with error (absolute)\")\n",
    "plt.colorbar(sc)\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "# the np.newaxis need are required to have 2D arrays:\n",
    "norm_factor = np.max(\n",
    "    [\n",
    "        np.max(pdist(X_test[0, :][:, np.newaxis])),\n",
    "        np.max(pdist(X_test[1, :][:, np.newaxis])),\n",
    "        np.max(pdist(X_test[2, :][:, np.newaxis])),\n",
    "    ]\n",
    ")  # take max. distance in test dataset as the norming factor\n",
    "\n",
    "sc = ax.scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    X_test[:, 2],\n",
    "    vmin=0,\n",
    "    vmax=0.1,\n",
    "    c=error_color / norm_factor,\n",
    "    cmap=\"Reds\",\n",
    ")\n",
    "plt.colorbar(sc)\n",
    "plt.title(\"test data with error (relative)\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that particularly at the edges the error is largest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
