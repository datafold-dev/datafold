{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Harmonics\n",
    "\n",
    "The Diffusion Map algorithm reduces the ambient dimension of a (finite) dataset $X$ sampled on a manifold by embedding it into the space of leading eigenfunctions of the Laplacian on the manifold. This can be used for dimension reduction of $X$ (as in diffusion_maps.ipynb) but also to *extend functions* which are defined on data $X$, i.e. $f(X): X \\rightarrow \\mathbb{R}$. Extending means that we are able to evaluate the function $f$ \"outside\" of $X$, e.g. in a neighborhood, for points $x_{new} \\notin X$. We can, therefore, use Diffusion Maps for interpolation; even when the dimensionality of $X$ is large. The points $x_{new}$ are generally referred to as \"out-of-sample\", and more specifically for Diffusion Maps as the Nyström method/extension. The Nyström method allows to extend functions $f(X)$ using a special set of (basis-)functions defined on $X$, the so-called *geometric harmonics* (see [2], from page 49).\n",
    "\n",
    "One application of the Nyström extension is to extend the eigenfunctions obtained from a subset of the data to the whole data set. The general (informal) workflow to describe the eigenfunctions on data is:\n",
    "\n",
    "* Collect data $X \\in \\mathbb{R}^n $ which is of high dimension and expected to be of lower intrinsic dimension\n",
    "* Apply Diffusion Maps: this gives us the new parametrization (=eigenvectors of kernel matrix) $\\Phi \\in \\mathbb{R}^m$, such that $m < n$ (as in diffusion_maps.ipynb). Each of the $m$ vectors (i.e. evaluations of the eigenfunction) is a function defined on $X$ (which give a new parametrization in $\\mathbb{R}^m$).\n",
    "* We now obtain new data $X_{new} \\notin X$ and we are interested to get the corresponding $\\Phi_{new}$ values for $ X_{new})$. A straightforward approach is to re-compute all data $[X, X_{new}]$. This, however, has potentially unwanted side effects: the parameterization values of already reduced values $X$ change and the `epsilon` value may not be correct anymore. In the examples below, we use the *geometric harmonics* to extend the eigenfunctions on the data.     \n",
    "\n",
    "#### Issues purposely left out to maintain a clear structure in the notebook:\n",
    "* When applying Diffusion Maps for dimensional reduction, forward and inverse mapping (mappings are described below), there are three different (and optimal) `epsilon` values. These can be either set heuristically or can be optimized (e.g. using the residual for interpolation). More sophisticated methods are k-fold cross-validation.\n",
    "* The number of eigenpairs can also be optimized. Usually, the more the better, however, there are also numerical issues at some point (see [2]).\n",
    "\n",
    "\n",
    "[1] Yoshua Bengio et al., Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering, https://papers.nips.cc/paper/2461-out-of-sample-extensions-for-lle-isomap-mds-eigenmaps-and-spectral-clustering.pdf\n",
    "\n",
    "[2] Stephane Lafon, Diffusion Maps and Geometric Harmonics, PhD thesis, Yale University, U.S.A., 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from tutorial_progress import TutorialBar\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "# NOTE: make sure \"path/to/datafold\" is in sys.path or PYTHONPATH if not installed\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "from datafold.dynfold import GeometricHarmonicsInterpolator as GHI\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step: Define functions on available data\n",
    "We compute the eigenvectors (as a finite representation of eigenfunctions) and use them as a new parametrization (here, for the swiss roll example, using $\\phi_1$ and $\\phi_5$, as in diffusion_maps.ipynb). In the subsequent steps, we want to interpolate the eigenvectors $\\phi_{1,5}$ outside the available data $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain dataset with a lot of points to get accurate eigenfunctions\n",
    "np.random.seed(random_state)\n",
    "X_all, color = make_swiss_roll(n_samples=20000, noise=0, random_state=random_state)\n",
    "\n",
    "num_eigenpairs=6\n",
    "\n",
    "pcm = pfold.PCManifold(X_all)\n",
    "pcm.optimize_parameters()\n",
    "\n",
    "dmap = dfold.DiffusionMaps(epsilon=pcm.kernel.epsilon, cut_off=pcm.cut_off, n_eigenpairs=num_eigenpairs)\n",
    "dmap = dmap.fit(pcm)\n",
    "evecs, evals = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "\n",
    "# new parametrization, used as \"ground truth\" (see diffusion_maps.ipynb for why the 1st and 5th is used)\n",
    "phi_all = evecs[:,[1, 5]]\n",
    "\n",
    "# select a subset of the data to proceed, so that the next computations are less expensive\n",
    "ind_subset = np.sort(np.random.permutation(np.arange(X_all.shape[0]))[0:2500])\n",
    "X_all = X_all[ind_subset,:]\n",
    "phi_all = phi_all[ind_subset,:]\n",
    "color = color[ind_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot:\n",
    "fig = plt.figure(figsize=[10, 5])\n",
    "\n",
    "fig.suptitle(f\"total #samples={X_all.shape[0]}\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "ax.set_title(\"all data, original coordinates\")\n",
    "ax.scatter(*X_all.T, s=5, c=color, cmap=plt.cm.Spectral)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "ax = fig.add_subplot(1,2, 2)\n",
    "ax.set_title(\"all data, diffusion map coordinates\")\n",
    "ax.scatter(*phi_all.T, s=5, c=color, cmap=plt.cm.Spectral);\n",
    "ax.set_xlabel(r'$\\phi_1$')\n",
    "ax.set_ylabel(r'$\\phi_5$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step: Setting up data for geometric harmonic interpolation \n",
    "\n",
    "We split the data $X$ and target values $\\Phi$ (computed in step 1) into training and test set. For the training set we have $\\{X_i, f(X_i)\\}_{i=1}^{N_{train}}$ for randomly sampled $X$ values. Later, we want to find the corresponding $f(X_j^{(test)}) = \\phi_j^{(1,5)} $. We compute the Diffusion Map coordinates on the training data, which are required for the interpolation.\n",
    "\n",
    "Remarks:\n",
    "* 1: The Diffusion Map computed on $X^{(train)}$ for interpolation are used as a (functional) basis. This means we take the eigenvectors ordered by the corresponding eigenvalue and we do not need to select the coordinates as for the dimensional reduction\n",
    "* 2: it is highly recommended to *not* use a normalized kernel (larger errors were observed in the experiments)\n",
    "* 3: in the code (folder `dm`), *only* the sparse version of Diffusion Maps works currently for *GeometricHarmonicInterpolator*. To have no side-effects, the `cut_off` is set to infinity to mimic the case of dense evaluation.\n",
    "* 4: for the interpolation basis, we usually need more eigenpairs than for dimensional reduction (here, 100 instead of 2)\n",
    "* 5: from point 4 follows that the epsilon values are more suitable at larger scales (in the example: 100 for interpolation vs. 1.25 for dimensional reduction) \n",
    "* 6: the eigenvalues behave $\\lambda_i \\rightarrow 0, \\text{ for } i \\rightarrow \\infty$. The interpolation using geometric harmonics involve a term $1/\\lambda_i$. This means, with too many eigenpairs, instabilities/numerical issues show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a random subset of the data and parametrization repectivly\n",
    "# the phi_test values are used for the \"ground truth\" values to measure an error\n",
    "# the color values are used for plotting\n",
    "X_train, X_test, phi_train, phi_test, color_train, color_test = train_test_split(X_all, phi_all, color, train_size=2/3)\n",
    "\n",
    "# compute the geometric harmonics from X to phi\n",
    "num_eigenpairs = 100\n",
    "\n",
    "# estimate resonable epsilon and cut_off values automatically\n",
    "pcm = pfold.PCManifold(X_train)\n",
    "pcm.optimize_parameters()\n",
    "\n",
    "# construct the GeometricHarmonicsInterpolator and fit it to the data.\n",
    "gh_interpolant = GHI(epsilon=pcm.kernel.epsilon**2, cut_off=pcm.cut_off,\n",
    "                     n_eigenpairs=num_eigenpairs, symmetrize_kernel=False)\n",
    "\n",
    "gh_interpolant.fit(pcm, phi_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Step: Compute the interpolation functions\n",
    "Use geometric harmonics from previously computed Diffusion Map. These can be used as geometric harmonics for interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute residual of interpolation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(tr, ap):  # also used later to compute the error\n",
    "    return np.sqrt(np.mean((tr-ap)**2))\n",
    "\n",
    "phi_train_approx = gh_interpolant.predict(X_train)\n",
    "res0 = compute_error(phi_train_approx, phi_train)\n",
    "\n",
    "phi_test_approx = gh_interpolant.predict(X_test)\n",
    "res1 = compute_error(phi_test_approx, phi_test)\n",
    "\n",
    "print(f\"residual train: {res0}, residual test: {res1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Step: Evaluate interpolation function\n",
    "\n",
    "On the left side, we plot the \"ground truth\" function values. On the right side, we evaluate the interpolation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=[10, 5],sharey=True)\n",
    "\n",
    "ax[0].set_title(\"ground truth test set ${\\phi}_{1,5}$\")\n",
    "ax[0].scatter(phi_test[:, 0], \n",
    "           phi_test[:, 1], s=10, c=color_test, cmap=plt.cm.Spectral)\n",
    "ax[0].set_xlabel(r'$\\phi_1$')\n",
    "ax[0].set_ylabel(r'$\\phi_5$')\n",
    "\n",
    "ax[1].scatter(*phi_test.T, s=10, c=color_test, cmap=plt.cm.Spectral)\n",
    "ax[1].set_title(r\"interpolated values $\\hat{\\phi}_{1,5}$\");\n",
    "ax[1].set_xlabel(r'$\\hat{\\phi}_1$')\n",
    "ax[1].set_ylabel(r'$\\hat{\\phi}_5$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step: Measure and plot error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error1 = compute_error(phi_test[:, 0], phi_test_approx[:,0]) \n",
    "error2 = compute_error(phi_test[:, 1], phi_test_approx[:,1])\n",
    "print(f\"error phi0: {error1}, error phi1: {error2}\")\n",
    "\n",
    "error_color = (phi_test[:, 0]-phi_test_approx[:,0])**2 + (phi_test[:, 1] - phi_test_approx[:,1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=[12, 5],sharey=True)\n",
    "\n",
    "sc = ax[0].scatter(phi_test[:, 0], phi_test[:, 1], c=error_color, cmap=\"Reds\")\n",
    "ax[0].set_title(\"test data with error (absolute)\")\n",
    "plt.colorbar(sc,ax=ax[0]);\n",
    "ax[0].set_xlabel(r'${\\phi}_1$')\n",
    "ax[0].set_ylabel(r'${\\phi}_5$')\n",
    "\n",
    "# the np.newaxis need are required to have 2D arrays:\n",
    "norm_factor = np.max([np.max(pdist(phi_test[0, :][:, np.newaxis])), \n",
    "                      np.max(pdist(phi_test[1, :][:, np.newaxis]))])   # take max. distance in test dataset as the norming factor\n",
    "\n",
    "sc = ax[1].scatter(phi_test[:, 0], phi_test[:, 1], vmin=0, vmax=.1, c=error_color/norm_factor, cmap=\"Reds\")\n",
    "plt.colorbar(sc,ax=ax[1]);\n",
    "ax[1].set_title(f\"test data with error (relative)\");\n",
    "ax[1].set_xlabel(r'${\\phi}_1$')\n",
    "ax[1].set_ylabel(r'${\\phi}_5$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Step: Geometric Harmonics to interpolate inverse of dimensional reduction transformation\n",
    "\n",
    "This step basically repeats the above steps (except step 1, we use the same data samples), however, this time we want to interpolate $X$ values defined on $\\Phi$ values. This is $f^{-1}: \\Phi \\rightarrow X $ and therefore we obtain the inverse mapping of the dimensional reduction (carried out in step 1). We have to compute another Diffusion Maps, but this time on the $\\Phi$ values as a dataset. We then interpolate all original coordinates as function values on $\\Phi$. There are three dimensions in the original space of the swiss roll, so we need three interpolation functions.\n",
    "\n",
    "* Because the Euclidean distances are much smaller in the reduced $\\Phi$ space, we need a much smaller `epsilon` value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle again new sets.\n",
    "# NOTE: this time, the \"ground truth\" values are X_test (-> target values)\n",
    "\n",
    "print('Generating test/train split...')\n",
    "X_train, X_test, phi_train, phi_test, color_train, color_test = train_test_split(X_all, phi_all,\\\n",
    "                                                                color, train_size=2/3, random_state=random_state)\n",
    "\n",
    "print('Computing bounds for parameters...')\n",
    "pcm = pfold.PCManifold(phi_train) # (!!) we use the dmap space as base space now, and transform to X\n",
    "pcm.optimize_parameters(random_state=random_state)\n",
    "\n",
    "opt_epsilon = pcm.kernel.epsilon\n",
    "opt_cutoff = pcm.cut_off\n",
    "opt_n_eigenpairs = 100\n",
    "\n",
    "# test the interpolation quality without optimization (which is the next cell)\n",
    "\n",
    "gh_interpolant_phi_to_X = GHI(epsilon=opt_epsilon,\\\n",
    "            cut_off=opt_cutoff, n_eigenpairs=opt_n_eigenpairs,\\\n",
    "            symmetrize_kernel=True, is_stochastic=False)\n",
    "gh_interpolant_phi_to_X.fit(phi_train,X_train)\n",
    "\n",
    "res0 = compute_error(X_test, gh_interpolant_phi_to_X.predict(phi_test))\n",
    "print(f\"residual without further parameter optimization: {res0}\")\n",
    "\n",
    "# plot ground truth and interpolated values\n",
    "fig = plt.figure(figsize=[16, 9])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"ground truth values\")\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$')\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "ax.scatter(*(gh_interpolant_phi_to_X.predict(phi_test)).T,\n",
    "           c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"interpolated values (no hyperparameter optimization)\");\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# Bayesian optimization using Gaussian processes with skopt.\n",
    "# Note that GHI scores are the mean squared error, so we want to maximize the negative score.\n",
    "# Also note that cross-validation is tricky with kernel methods, as reducing the data set also\n",
    "# changes which parameters are \"optimal\". Here, we use the entire training set to find the parameters.\n",
    "opt = BayesSearchCV(\n",
    "    GHI(symmetrize_kernel=True, is_stochastic=False),\n",
    "    {\n",
    "        'epsilon': Real(pcm.kernel.epsilon/5, pcm.kernel.epsilon*5, prior='log-uniform'),\n",
    "        'cut_off': Real(pcm.cut_off/2, pcm.cut_off*2, prior='uniform'),\n",
    "        'n_eigenpairs': Integer(100, 1000, prior='uniform'),\n",
    "    },\n",
    "    n_iter=n_iters,\n",
    "    random_state=0,\n",
    "    scoring=lambda estimator,x,y: -np.sum(estimator.score(x,y)),           # is to be maximized\n",
    "    cv=[[np.random.permutation(X_train.shape[0]),                          # train indices\n",
    "         np.random.permutation(X_train.shape[0])[0:X_train.shape[0]//10]]] # test indices\n",
    ")\n",
    "\n",
    "# display a progress bar\n",
    "bar = TutorialBar(n_iters)\n",
    "\n",
    "# run the optimization\n",
    "opt.fit(phi_train, X_train, callback=lambda info: bar.update())\n",
    "\n",
    "# get the results\n",
    "optimal_GHI = opt.best_estimator_\n",
    "print(f'Previous epsilon: {pcm.kernel.epsilon}, cut-off: {pcm.cut_off}, #eigenpairs: {num_eigenpairs}')\n",
    "print(f'Optimal epsilon: {optimal_GHI.epsilon}, cut-off: {optimal_GHI.cut_off}, #eigenpairs: {optimal_GHI.n_eigenpairs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute residual for best model\n",
    "res0 = compute_error(X_test, optimal_GHI.predict(phi_test))\n",
    "\n",
    "print(f\"residual X0: {res0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot information about the Geometric Harmonics (i.e. eigenvalues and eigenvectors)\n",
    "fig,ax=plt.subplots(1,1)\n",
    "ax.semilogy(optimal_GHI.eigenvalues_,'.-')\n",
    "ax.set_xlabel('#eigenvalue')\n",
    "ax.set_ylabel(r'$\\lambda$')\n",
    "\n",
    "fig,ax = plt.subplots(2,6,figsize=[10,3],sharex=True,sharey=True)\n",
    "for k1 in range(ax.shape[0]):\n",
    "    for k2 in range(ax.shape[1]):\n",
    "        index = k2 + k1*ax.shape[1]\n",
    "        ax[k1,k2].scatter(*phi_train.T,s=10, c=optimal_GHI.eigenvectors_[:,index], cmap=plt.cm.Spectral)\n",
    "        ax[k1,k2].set_title(r'$\\phi_{' + str(index) + '}$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth and interpolated values\n",
    "fig = plt.figure(figsize=[16, 9])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"ground truth values\")\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$')\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "ax.scatter(*(optimal_GHI.predict(phi_test)).T,\n",
    "           c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"interpolated values\");\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot error\n",
    "error1 = compute_error(X_test[:, 0], optimal_GHI.predict(phi_test)[:,0]) \n",
    "error2 = compute_error(X_test[:, 1], optimal_GHI.predict(phi_test)[:,1])\n",
    "error3 = compute_error(X_test[:, 2], optimal_GHI.predict(phi_test)[:,2])\n",
    "print(f\"error X0: {error1}, error X1: {error2}, error X2: {error3}\")\n",
    "\n",
    "error_color = np.linalg.norm(X_test - optimal_GHI.predict(phi_test),axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=[16, 7])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "sc = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=error_color, cmap=\"Reds\")\n",
    "plt.title(\"test data with error (absolute)\")\n",
    "plt.colorbar(sc);\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$')\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "# the np.newaxis need are required to have 2D arrays:\n",
    "norm_factor = np.max([np.max(pdist(X_test[0, :][:, np.newaxis])), \n",
    "                      np.max(pdist(X_test[1, :][:, np.newaxis])), \n",
    "                      np.max(pdist(X_test[2, :][:, np.newaxis])) ])   # take max. distance in test dataset as the norming factor\n",
    "\n",
    "sc = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], vmin=0, vmax=.1, c=error_color / norm_factor, cmap=\"Reds\")\n",
    "plt.colorbar(sc);\n",
    "plt.title(f\"test data with error (relative)\");\n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')\n",
    "ax.set_zlabel(r'$x_3$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
