{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swiss roll hyperparameter estimation example\n",
    "\n",
    "# import all necessary packages\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "from skopt import gp_minimize\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "\n",
    "from sklearn.datasets import make_swiss_roll, make_s_curve\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.manifold as manifold\n",
    "\n",
    "from tutorial_progress import TutorialBar\n",
    "\n",
    "# NOTE: make sure \"path/to/datafold\" is in sys.path or PYTHONPATH if not installed\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "nr_samples = 1000\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# generating the nonlinear dataset \n",
    "nonlinear_data, color_nonlinear = make_s_curve(nr_samples, noise=0)  # using scikit learn package\n",
    "\n",
    "plot_idx= np.random.permutation(nr_samples)[0:2000]\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(nonlinear_data[plot_idx, 0], nonlinear_data[plot_idx, 1], nonlinear_data[plot_idx, 2],\n",
    "           c=color_nonlinear[plot_idx], cmap=plt.cm.Spectral) \n",
    "ax.set_xlabel(\"x\"); ax.set_ylabel(\"y\"); ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"nonlinear data point cloud\");\n",
    "ax.view_init(10,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the PCManifold, optimize hyperparameters, compute diffusion maps.\n",
    "\n",
    "pcm = pfold.PCManifold(nonlinear_data)\n",
    "pcm.optimize_parameters()\n",
    "\n",
    "def loss_dmaps(params):\n",
    "    eps = np.exp(params[0])\n",
    "    cut_off = params[1]\n",
    "\n",
    "    dmap = dfold.DiffusionMaps(epsilon=eps, cut_off=cut_off, n_eigenpairs=10)\n",
    "    dmap = dmap.fit(pcm)\n",
    "    evecs, evals = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "    \n",
    "    # one possibility to evaluate the \"quality\" of a DMAP embedding\n",
    "    # is to try to map back to the original space (here, pcm)\n",
    "    c,res,_,_ = np.linalg.lstsq(evecs, pcm, rcond=1e-10)\n",
    "    return float(np.sum(res))\n",
    "\n",
    "n_iters_dmap = 25\n",
    "bounds_dmaps = np.array([[np.log(pcm.kernel.epsilon/5),np.log(pcm.kernel.epsilon*5)],\\\n",
    "                         [pcm.cut_off/2,pcm.cut_off*2]])\n",
    "\n",
    "# the minimization throws some unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "bar = TutorialBar(n_iters_dmap)\n",
    "res = gp_minimize(loss_dmaps,           # the function to minimize\n",
    "                  bounds_dmaps,         # the bounds on each dimension of x\n",
    "                  acq_func=\"EI\",        # the acquisition function\n",
    "                  n_calls=n_iters_dmap, # the number of evaluations of the loss\n",
    "                  n_random_starts=5,    # the number of random initialization points\n",
    "                  random_state=1234,    # the random seed\n",
    "                  callback=lambda state: bar.update()) # the progress bar\n",
    "xp,yp = np.row_stack(res.x_iters), np.row_stack(res.func_vals)\n",
    "\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "ind_best = np.argmin(yp)\n",
    "xp_best = xp[ind_best,:]\n",
    "\n",
    "opt_epsilon = np.exp(xp_best[0])\n",
    "opt_cutoff = xp_best[1]\n",
    "\n",
    "print(f'Previously: epsilon={pcm.kernel.epsilon}, cut-off={pcm.cut_off}')\n",
    "print(f'Optimal: epsilon={opt_epsilon}, cut-off={opt_cutoff}')\n",
    "\n",
    "# after the optimal parameters have been found, we can construct DMAP\n",
    "dmap = dfold.DiffusionMaps(epsilon=opt_epsilon, cut_off=opt_cutoff, n_eigenpairs=10)\n",
    "dmap = dmap.fit(pcm)\n",
    "evecs, evals = dmap.eigenvectors_, dmap.eigenvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,5,figsize=(12, 3),sharey=True)\n",
    "for k in range(5):\n",
    "    ax[k].scatter(evecs[plot_idx, 1], evecs[plot_idx,2+k],s=5, c=color_nonlinear[plot_idx], cmap='viridis') \n",
    "    ax[k].set_xlabel(r\"$\\phi_1$\"); ax[k].set_title(r\"$\\phi_%g$\" % (k+2));\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now sample the parameters on a grid to be able to compare the hyperparameter results\n",
    "eps_space = np.linspace(bounds_dmaps[0][0],bounds_dmaps[0][1],5)\n",
    "cutoff_space = np.linspace(bounds_dmaps[1][0],bounds_dmaps[1][1],5)\n",
    "epss,cutoffs = np.meshgrid(eps_space,cutoff_space)\n",
    "loss_dmaps_all = np.zeros(epss.shape)\n",
    "\n",
    "bar = TutorialBar(np.prod(epss.shape))\n",
    "for k1 in range(epss.shape[0]):\n",
    "    for k2 in range(epss.shape[1]):\n",
    "        loss_dmaps_all[k1,k2] = loss_dmaps([epss[k1,k2], cutoffs[k1,k2]])\n",
    "        bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cubic interpolation to get a smoother picture\n",
    "points = np.column_stack([epss.ravel(), cutoffs.ravel()])\n",
    "\n",
    "eps_space2 = np.linspace(bounds_dmaps[0][0],bounds_dmaps[0][1],50)\n",
    "cutoff_space2 = np.linspace(bounds_dmaps[1][0],bounds_dmaps[1][1],50)\n",
    "epss2,cutoffs2 = np.meshgrid(eps_space2,cutoff_space2)\n",
    "\n",
    "grid_loss = scipy.interpolate.griddata(points, -np.log(loss_dmaps_all.ravel()),\\\n",
    "                     (epss2, cutoffs2),  method='cubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "c = -np.log(yp+1e-10)\n",
    "c = np.arange(0,xp.shape[0])/2+1\n",
    "\n",
    "fig,ax=plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].imshow(grid_loss,\\\n",
    "             extent=[np.min(eps_space),np.max(eps_space),np.min(cutoff_space),np.max(cutoff_space)],\\\n",
    "             origin='lower', cmap='viridis')\n",
    "ax[0].scatter(*xp.T,s=np.arange(0,xp.shape[0])+10,c=c,cmap='bwr')\n",
    "ax[0].plot(*xp_best.T,'gx')\n",
    "ax[0].set_title('sampling in parameter space')\n",
    "ax[0].set_xlabel(r'$\\log(\\epsilon)$')\n",
    "ax[0].set_ylabel(r'cut-off')\n",
    "ax[0].set_aspect((np.max(eps_space)-np.min(eps_space)) / (np.max(cutoff_space)-np.min(cutoff_space)))\n",
    "\n",
    "ax[1].plot(np.log(yp))\n",
    "ax[1].plot(ind_best,np.log(yp[ind_best]),'gx')\n",
    "ax[1].set_xlabel('iteration')\n",
    "ax[1].set_ylabel(r'$\\log$(error)');\n",
    "ax[1].set_title('error over iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
