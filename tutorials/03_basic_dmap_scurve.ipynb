{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Visit the\n",
    "[documentation](https://datafold-dev.gitlab.io/datafold/tutorial_index.html) page\n",
    "to view the executed notebook.)\n",
    "\n",
    "# Diffusion Maps: Embedding of an S-curved manifold\n",
    "\n",
    "For a detailed introduction see paper \n",
    "\n",
    "Coifman and Lafon, Diffusion maps, Appl. Comput. Harmon. Anal. 21 (2006) 5â€“30, 2006, Available at: https://www.sciencedirect.com/science/article/pii/S1063520306000546\n",
    "\n",
    "Diffusion Maps is an algorithm to \"learn\" (i.e. parametrize) a manifold from data (cf. \"manifold learning\"). The usual assumption is that the original point cloud is represented in a high-dimensional space (the ambient space), while the points lie close to a manifold with an intrinsically lower-dimension. Using the Diffusion Map algorithm we aim to parametrize this (hidden) manifold to obtain a parsimonious data representation.\n",
    "\n",
    "The `DiffusionMaps` algorithm allows embedding the points into a lower-dimension and at the same time aims to preserve some quantities of interest such as local mutual distances. The Diffusion Map algorithm constructs a Markov Chain based on the available point cloud - the probabilities describe a diffusion process on the point cloud. The probabilities of the Markov Chain encode the locality: how probable a transition between two data points is in one time step of the process is. Finally, the eigenvectors of the Markov Chain matrix are the stationary solution with $t \\rightarrow \\infty$ and encode the new parsimonious representation.\n",
    "\n",
    "Alternative manifold learning methods are, for example: Isomaps, Local Linear Embedding or Hessian eigenmaps. For a quick comparison, (without Diffusion Maps). See e.g.\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py\n",
    "\n",
    "**In this tutorial** we use the `PCManifold` to select suitable parameters (`epsilon` and `cut_off`), fit a `DiffusionMaps` model to learn the S-curved manifold (like in the scikit-learn link above) and in the last step we show how to find the \"right\" parsimonious representation automatically with `LocalRegressionSelection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import numpy as np\n",
    "import sklearn.manifold as manifold\n",
    "from sklearn.datasets import make_s_curve, make_swiss_roll\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "from datafold.dynfold import LocalRegressionSelection\n",
    "from datafold.utils.plot import plot_pairwise_eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate S-curved point cloud  \n",
    "\n",
    "We use the generator `make_s_curve` from scikit-learn. The points have a three-dimensional representation (ambient space) and the points lie on a (non-linear) S-curved shaped manifold with intrinsic dimension two (i.e. on the folded plane). The scikit-learn function also provides pseudo-colouring, which allows to better visualize different embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples = 15000\n",
    "\n",
    "# reduce number of points for plotting\n",
    "nr_samples_plot = 1000\n",
    "idx_plot = np.random.permutation(nr_samples)[0:nr_samples_plot]\n",
    "\n",
    "# generate point cloud\n",
    "X, X_color = make_s_curve(nr_samples, random_state=3, noise=0)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    X[idx_plot, 0],\n",
    "    X[idx_plot, 1],\n",
    "    X[idx_plot, 2],\n",
    "    c=X_color[idx_plot],\n",
    "    cmap=plt.cm.Spectral,\n",
    ")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"point cloud on S-shaped manifold\")\n",
    "ax.view_init(10, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize kernel parameters \n",
    "\n",
    "We now use a `PCManifold` to estimate parameters. The attached kernel in `PCManifold` defaults to a `GaussianKernel`.\n",
    "\n",
    "* `epsilon` - the scale of the Gaussian kernel\n",
    "* `cut_off` - promotes kernel sparsity and allows the number of samples in a dataset to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pcm = pfold.PCManifold(X)\n",
    "X_pcm.optimize_parameters()\n",
    "\n",
    "print(f\"epsilon={X_pcm.kernel.epsilon}, cut-off={X_pcm.cut_off}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit DiffusionMaps model \n",
    "\n",
    "We first fit a `DiffusionMaps` model with the optimized parameters and then compare potential two-dimensional embeddings. For this we fix the first non-trivial eigenvector ($\\psi_1$) and compare it to the other computed eigenvectors ($\\{\\psi\\}_{i=1, i \\neq 1}^{9}$). \n",
    "\n",
    "**Observations:**\n",
    "\n",
    "* The first eigenvector $\\psi_0$ (with eigenvalue 1) can usually be ignored if `is_stochastic=True` as it is constant (cf. a property of row-stochastic matrix).\n",
    "* Pairings like $\\psi_1$ and $\\psi_2$ are good examples for a \"functional dependence\" between eigenvectors. Our aim for the S-curve is to unfold in a two-dimensional coordinate space. For pairings with a functional dependence, the embeddings are poor because the eigenvector ($\\rightarrow$ eigenfunction) does not go along a new and independent direction compared to eigenvector $\\psi_1$.  \n",
    "* We identify the eigenvector pair $\\psi_1$ and $\\psi_5$ as the best choice. It does a good job to \"unfold\" the two dimensional S-curved manifold into now two-dimensional embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmap = dfold.DiffusionMaps(\n",
    "    kernel=pfold.GaussianKernel(epsilon=X_pcm.kernel.epsilon),\n",
    "    n_eigenpairs=9,\n",
    "    dist_kwargs=dict(cut_off=X_pcm.cut_off),\n",
    ")\n",
    "dmap = dmap.fit(X_pcm)\n",
    "evecs, evals = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "\n",
    "plot_pairwise_eigenvector(\n",
    "    eigenvectors=dmap.eigenvectors_[idx_plot, :],\n",
    "    n=1,\n",
    "    fig_params=dict(figsize=[15, 15]),\n",
    "    scatter_params=dict(cmap=plt.cm.Spectral, c=X_color[idx_plot]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic embedding selection\n",
    "\n",
    "In the visual comparison, we can identify good choices if the dimension is low (two dimensional for plotting). For larger (intrinsic) dimensions of the manifold, this becomes much harder or impossible. Furthermore, we also wish to automatize this process.\n",
    "\n",
    "In a (linear) Principal Component Analysis (PCA), the eigenvectors are sorted by relevance and each eigenvector points in the direction of (next) larger variance. This is a property by construction and because of this intrinsic order, the selection process is simpler as we only have to look at the magnitude of corresponding eigenvalues or a gap in the eigenvalues. \n",
    "\n",
    "For manifold learning models like `DiffusionMaps` we trade-off lower-dimensional embeddings (by accounting for non-linearity) with a harder selection process. The eigenvectors with large eigenvalue may not add new information compared to the previous eigenvectors (as in the plot above). For an automatic selection of suitable eigenvector coordinates, we need a model that quantifies what a \"good\" embedding is and optimize for this quantity. \n",
    "\n",
    "An automatic selection model is provided in `datafold.dynfold.LocalRegressionSelection`. It provides two strategies (\"fixed dimension\" or \"threshold\"). Because we know *apriori* that the intrinsic dimension is two, we choose this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = LocalRegressionSelection(\n",
    "    intrinsic_dim=2, n_subsample=500, strategy=\"dim\"\n",
    ").fit(dmap.eigenvectors_)\n",
    "print(f\"Found parsimonious eigenvectors (indices): {selection.evec_indices_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the selection-model was able to find the same eigenvector pairs for embedding that we identified as the best choice before. Finally, we plot the unfolded point cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mapping = selection.transform(dmap.eigenvectors_)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 9))\n",
    "ax.scatter(\n",
    "    target_mapping[idx_plot, 0],\n",
    "    target_mapping[idx_plot, 1],\n",
    "    c=X_color[idx_plot],\n",
    "    cmap=plt.cm.Spectral,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
