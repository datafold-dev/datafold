{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a053711",
   "metadata": {},
   "source": [
    "# Dictionary Learning for enhanced Koopman Operator approximations\n",
    "\n",
    "Original paper: Li, Qianxiao, et al. \"Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.10 (2017). https://doi.org/10.1063/1.4993854\n",
    "\n",
    "The conventional fixed dictionary approach in EDMD can pose challenges, particularly when dealing with high-dimensional and nonlinear systems. To overcome this limitation, the above paper proposes an advancement using dictionary learning techniques. By combining EDMD with a trainable artificial neural network dictionary, the EDMD-DL can dynamically adapt the observables without the need for preselection. This notebook repeats the demonstrates the functionality in datafold by repeating the Duffing oscillator case of the paper (Section IV-A).\n",
    "\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- The implementation is in an early stage. This means that the API and class names may change if needed.\n",
    "- The neural network is specified in `torch`, which needs to be installed separately from the datafold's dependencies\n",
    "- Currently the neural network does not make use of GPU computations (contributions welcome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from datafold import EDMD, TSCIdentity\n",
    "from datafold.dynfold.dictlearning import DMDDictLearning, FeedforwardNN\n",
    "from datafold.utils._systems import Duffing\n",
    "from datafold.utils.plot import plot_eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d89f5",
   "metadata": {},
   "source": [
    "## Specify original system and sample data\n",
    "\n",
    "In line with the original paper we parametrize the Duffing system. The resulting system has two stable steady states at $(\\pm1,0)$ separated by a saddle point at $(0, 0)$. By collecting data we convert the continuous dynamical system to a discrete one by defining a flow map mapping from one state to the next with a fixed time interval `dt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.5\n",
    "beta = -1.0\n",
    "alpha = 1.0\n",
    "\n",
    "system = Duffing(alpha=alpha, beta=beta, delta=delta)\n",
    "\n",
    "dt = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce9729",
   "metadata": {},
   "source": [
    "### Specify fraining and test data\n",
    "For the training we draw 1000 random initial conditions uniformly in the region $[-2,2]^2$⁠. Each initial condition is evolved up to `num_steps = 10` with the flow-map so that we have a total of $10^5$ data points to form the training set.\n",
    "\n",
    "In addition (not covered in the paper), we define two out-of-sample trajectories. where one starts in the lower left corner $[-2,2]$ and upper right corner $[2,2]$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "num_init = 1000\n",
    "num_steps = 10\n",
    "\n",
    "time_values = np.arange(0, dt * num_steps, dt)\n",
    "rng = np.random.default_rng(2)\n",
    "IC = rng.uniform(low=[-2, -2], high=[2, 2], size=(num_init, 2))\n",
    "\n",
    "X, _ = system.predict(IC, time_values=time_values)\n",
    "\n",
    "# test data\n",
    "time_values_oos = np.arange(0, dt * 500, dt)\n",
    "IC_oos = np.array([[-2, -2], [2, 2]], dtype=np.float64)\n",
    "\n",
    "X_oos, _ = system.predict(IC_oos, time_values=time_values_oos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4833cd",
   "metadata": {},
   "source": [
    "### Characteristics of data stored in TSCDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2507497",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X.n_timeseries=}\")\n",
    "print(f\"{X.n_timesteps=}\")\n",
    "print(f\"{X.delta_time=}\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_oos.n_timeseries=}\")\n",
    "print(f\"{X_oos.n_timesteps=}\")\n",
    "print(f\"{X_oos.delta_time=}\")\n",
    "X_oos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568ebe9",
   "metadata": {},
   "source": [
    "### Plot both training and test trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "for i, df in X.itertimeseries():\n",
    "    X_np = df.to_numpy()\n",
    "    ax.plot(\n",
    "        X_np[:, 0],\n",
    "        X_np[:, 1],\n",
    "        c=\"black\",\n",
    "        linewidth=0.1,\n",
    "        label=\"training\" if i == 0 else None,\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Original Duffing system with ODE solver (training and test data)\")\n",
    "for i, df in X_oos.itertimeseries():\n",
    "    ax.plot(\n",
    "        df.iloc[:, 0].to_numpy(),\n",
    "        df.iloc[:, 1].to_numpy(),\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "        label=f\"test traj. {i}\",\n",
    "    )\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09d250",
   "metadata": {},
   "source": [
    "## Building EDMD model\n",
    "\n",
    "In this next step, we proceed with training the EDMD-DL model, leveraging the convenient `EDMD` class that supports a combination of fixed dictionary elements, such as time delay embedding, along with dictionary learning. For this study, we opt to use the identity for simplicity, which matches the case of the original paper.\n",
    "\n",
    "The core concept behind incorporating dictionary learning lies in the creation of a dedicated variant of dynamic mode decomposition, the `DMDDictLearning` class. This class not only learns observables from the data but also provides the mode decomposition of the system matrix. While various learning algorithms can be included in `DMDDictLearning`, the primary supported class is `FeedforwardNN`, which aligns with the specifications of Li et al.\n",
    "\n",
    "For our study, we specify the neural network with the same number of layers, width per layer, and output size (representing the number of observables). We train the network with a relatively low number of epochs, and additional training parameters can be passed to `fit_params`. In this case, we set a learning rate scheduler `ReduceLROnPlateau` from pytorch and utilize `X_oos` as validation data (impacting the scheduler). The losses are recorded to facilitate later training vs. validation loss visualization. The option to disable the `tqdm` progress bar and set an early stopping in `fit_params` is also available, but not highlighted in this tutorial.\n",
    "\n",
    "Finally, with both the dictionary learning steps and the dynamic mode decomposition (implemented through `DMDDictLearning`) incorporated into the standard `EDMD` class, we initiate the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0baf03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_steps = [(\"_id\", TSCIdentity())]\n",
    "network = FeedforwardNN(\n",
    "    hidden_size=100,\n",
    "    n_hidden_layer=3,\n",
    "    n_dict_elements=22,\n",
    "    batch_size=5000,\n",
    "    n_epochs=50,\n",
    "    sys_regularization=0.00,\n",
    "    learning_rate=1e-4,\n",
    "    random_state=1,\n",
    ")\n",
    "dmd = DMDDictLearning(learning_model=network)\n",
    "\n",
    "fit_params = dict(\n",
    "    dmd__record_losses=True,\n",
    "    dmd__X_val=X_oos,\n",
    "    dmd__lr_scheduler=ReduceLROnPlateau,\n",
    ")\n",
    "\n",
    "edmd = EDMD(\n",
    "    dict_steps=dict_steps,\n",
    "    dmd_model=dmd,\n",
    "    stepwise_transform=True,\n",
    "    include_id_state=False,\n",
    "    dict_preserves_id_state=False,\n",
    "    sort_koopman_triplets=False,\n",
    ")\n",
    "edmd.fit(X, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters {edmd[-2].n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c353d00-0980-4555-af53-4b6477d5e3b4",
   "metadata": {},
   "source": [
    "We now look at the EDMD instance again and see that the dictionary and final estimator changed during the model's fit. The `DMDDictLearning` provides both a transformer (in which the dictionary is learnt) as well as an DMD object for the predictions.\n",
    "\n",
    "In this case, the dictionary pipline (transformers) are now `TSCIdentity` and `FeedforwardNN`. This means when we evaluate `edmd.transform(X)`, we map X to the output layer of `FeedforwardNN`. Finally the estimator is a DMD class, which is predicting the dictionary states forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fb3f8-685c-45e1-971f-19aa4c530c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "edmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d249e2-c94b-4f92-b04c-c5f64e6d6727",
   "metadata": {},
   "source": [
    "### Plot training process of neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09dc5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(edmd[-2].fit_losses_, \"-*\", label=\"train error\")\n",
    "plt.semilogy(edmd[-2].val_losses_, \"-*\", c=\"orange\", label=\"val error\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e6678-89df-4223-b279-30e5441db5f2",
   "metadata": {},
   "source": [
    "### Evaluate EDMD model\n",
    "\n",
    "We reconstruct both the training data and out-of-sample data. We use these to compare the EDMD model against the Duffing system in plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reconstruct_train = edmd.reconstruct(X)\n",
    "X_oos_reconstruct = edmd.reconstruct(X_oos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166ae1a-c823-4734-8a6f-272e6b0d4038",
   "metadata": {},
   "source": [
    "We can also investigate the dictionary by mapping the data to the dictionary states. Here this corresponds to the original state, a constant and the last layer of the neural network (psis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab04da-647a-4039-8653-6a274a136991",
   "metadata": {},
   "outputs": [],
   "source": [
    "edmd.transform(X).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e08cc-47f7-4976-861c-778aac2763c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot comparison between EDMD-DL and Duffing system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cee680",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 5), ncols=2, sharex=True, sharey=True)\n",
    "\n",
    "for i, df in X.itertimeseries():\n",
    "    X_np = df.to_numpy()\n",
    "    ax[0].plot(\n",
    "        X_np[:, 0],\n",
    "        X_np[:, 1],\n",
    "        c=\"black\",\n",
    "        linewidth=0.1,\n",
    "        label=\"training\" if i == 0 else None,\n",
    "    )\n",
    "\n",
    "ax[0].set_title(\"Original Duffing system\")\n",
    "for i, df in X_oos.itertimeseries():\n",
    "    ax[0].plot(\n",
    "        df.iloc[:, 0].to_numpy(),\n",
    "        df.iloc[:, 1].to_numpy(),\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "        label=f\"test traj. {i}\",\n",
    "    )\n",
    "ax[0].legend()\n",
    "\n",
    "for _, df in X_reconstruct_train.itertimeseries():\n",
    "    X_np = df.to_numpy()\n",
    "    ax[1].plot(X_np[:, 0], X_np[:, 1], c=\"black\", linewidth=0.1)\n",
    "\n",
    "ax[1].set_title(\"Reconstructed with EDMD-DL\")\n",
    "\n",
    "for i, df in X_oos_reconstruct.itertimeseries():\n",
    "    ax[1].plot(\n",
    "        df.iloc[:, 0].to_numpy(),\n",
    "        df.iloc[:, 1].to_numpy(),\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "    )\n",
    "ax[0].grid()\n",
    "ax[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660911e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, sharex=True, sharey=True)\n",
    "for i, df in X_oos.itertimeseries():\n",
    "    ax[i].plot(\n",
    "        df.index[:100],\n",
    "        df.iloc[:100, 0].to_numpy(),\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "        label=f\"orig x {i}\",\n",
    "    )\n",
    "\n",
    "    ax[i].plot(\n",
    "        df.index[:100],\n",
    "        df.iloc[:100, 1].to_numpy(),\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "        label=f\"orig y {i}\",\n",
    "    )\n",
    "\n",
    "for i, df in X_oos_reconstruct.itertimeseries():\n",
    "    ax[i].plot(\n",
    "        df.index[:100],\n",
    "        df.iloc[:100, 0].to_numpy(),\n",
    "        \"--\",\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "        label=f\"pred x {i}\",\n",
    "    )\n",
    "\n",
    "    ax[i].plot(\n",
    "        df.index[:100],\n",
    "        df.iloc[:100, 1].to_numpy(),\n",
    "        \"--\",\n",
    "        c=[\"red\", \"blue\"][i],\n",
    "        label=f\"pred y {i}\",\n",
    "    )\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(\"time\")\n",
    "ax[0].set_ylabel(\"x/y\")\n",
    "ax[1].set_ylabel(\"x/y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13410ef",
   "metadata": {},
   "source": [
    "From the plots we can see that the predictions match the original system. By increasing the number of dictionary elements or by changing the learning process we may be able to further enhance the model's quality. \n",
    "\n",
    "Since the the underlying model in EDMD is linear, we can also view the eigenvalues and investigate their stability. This following plot compares to the analysis in Fig. 2 of Li et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_eigenvalues(edmd.koopman_eigenvalues.to_numpy(), plot_unit_circle=True)\n",
    "ax.grid();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
