{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Visit the\n",
    "[documentation](https://datafold-dev.gitlab.io/datafold/tutorial_index.html) page\n",
    "to view the executed notebook.)\n",
    "\n",
    "# RObust and Scalable Embedding via LANdmark Diffusion (Roseland)\n",
    "\n",
    "For a detailed introduction see paper \n",
    "\n",
    "Shen, Chao, and Hau-Tieng Wu. \"Scalability and robustness of spectral embedding: landmark diffusion is all you need.\" arXiv preprint arXiv:2001.00801 (2020). Available at: https://arxiv.org/abs/2001.00801\n",
    "\n",
    "The Roseland algorithm is a dimensionality reduction technique based on the manifold assumption (cf. \"manifold learning\", \"diffusion maps\"). It is motivated by the challenge to develop a scalable and robust algorithm capable of handling large datasets. The Roseland algorithm can be viewed as a generalization of the Diffusion Maps (DM) algorithm with which shares various properties. Its main advantage lies in mitigating the computational effort of computing the eigendecomposition of a large matrix containing the affinities between each two points in the dataset.\n",
    "\n",
    "Instead, the Roseland algorithm utlizies a \"landmark set\" in the computation of the affinity matrix. For large scale applications, this \"landmark set\" can be much smaller than the full dataset. This results in a rectangular matrix proportional to the size of the landmark set and full dataset to be decomposed with singular value decomposition, rather than the large full-dataset-square matrix of DM. It is also aparent than in the cases when the landmark coincides with the full dataset, the Roseland matches DM.\n",
    "\n",
    "**In this tutorial** we reimplement two of the earlier Diffusion Maps tutorials, [Embedding of an S-curved manifold](https://datafold-dev.gitlab.io/datafold/tutorial_03_basic_dmap_scurve.html) and [Manifold learning on handwritten digits](https://datafold-dev.gitlab.io/datafold/tutorial_04_basic_dmap_digitclustering.html).\n",
    "\n",
    "Alternative manifold learning methods are, for example: Isomaps, Local Linear Embedding or Hessian eigenmaps. For a quick comparison (without Diffusion Maps) see the [scikit-learn page](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import numpy as np\n",
    "import sklearn.manifold as manifold\n",
    "from matplotlib import offsetbox\n",
    "from sklearn.datasets import load_digits, make_s_curve, make_swiss_roll\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "from datafold.dynfold import LocalRegressionSelection\n",
    "from datafold.utils.plot import plot_pairwise_eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding of an S-curved manifold\n",
    "\n",
    "We use the `PCManifold` to select suitable parameters (`epsilon` and `cut_off`), fit a `Roseland` model to learn the S-curved manifold and in the last step we show how to find the \"right\" parsimonious representation automatically with `LocalRegressionSelection`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate S-curved point cloud  \n",
    "\n",
    "We use the generator `make_s_curve` from scikit-learn. The points have a three-dimensional representation (ambient space) and the points lie on a (non-linear) S-curved shaped manifold with intrinsic dimension two (i.e. on the folded plane). The scikit-learn function also provides pseudo-colouring, which allows to better visualize different embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples = 15000\n",
    "\n",
    "# reduce number of points for plotting\n",
    "nr_samples_plot = 1000\n",
    "idx_plot = np.random.permutation(nr_samples)[0:nr_samples_plot]\n",
    "\n",
    "# generate point cloud\n",
    "X, X_color = make_s_curve(nr_samples, random_state=3, noise=0)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    X[idx_plot, 0],\n",
    "    X[idx_plot, 1],\n",
    "    X[idx_plot, 2],\n",
    "    c=X_color[idx_plot],\n",
    "    cmap=plt.cm.Spectral,\n",
    ")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"point cloud on S-shaped manifold\")\n",
    "ax.view_init(10, 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize kernel parameters \n",
    "\n",
    "We now use a `PCManifold` to estimate parameters. The attached kernel in `PCManifold` defaults to a `GaussianKernel`.\n",
    "\n",
    "* `epsilon` - the scale of the Gaussian kernel\n",
    "* `cut_off` - promotes kernel sparsity and allows the number of samples in a dataset to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pcm = pfold.PCManifold(X)\n",
    "X_pcm.optimize_parameters()\n",
    "\n",
    "print(f\"epsilon={X_pcm.kernel.epsilon}, cut-off={X_pcm.cut_off}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Roseland model \n",
    "\n",
    "We first fit a `Roseland` model with the optimized parameters and then compare potential two-dimensional embeddings. For this we fix the first non-trivial svdvector ($\\psi_1$) and compare it to the other computed svdvectors ($\\{\\psi_i\\}_{i=1, i \\neq 1}^{9}$). \n",
    "\n",
    "**Comaprison with Diffusion Maps:**\n",
    "\n",
    "* Roseland makes use of similar metaparameters as the Diffusion maps, the major difference in the parameter `landmarks` which governs the size of the landmark set. Changing `landmarks` allows for a compromise between the accuracy and runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lm in (0.1, 0.25, 0.7):\n",
    "    rose = dfold.Roseland(\n",
    "        kernel=pfold.GaussianKernel(\n",
    "            epsilon=X_pcm.kernel.epsilon, distance=dict(cut_off=X_pcm.cut_off)\n",
    "        ),\n",
    "        n_svdtriplet=9,\n",
    "        landmarks=lm,\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    rose = rose.fit(X_pcm)\n",
    "    dt = time.time() - t0\n",
    "    svdvecs, svdvals = rose.svdvec_left_, rose.svdvalues_\n",
    "\n",
    "    plot_pairwise_eigenvector(\n",
    "        eigenvectors=svdvecs[idx_plot, :],\n",
    "        n=1,\n",
    "        fig_params=dict(figsize=[15, 15]),\n",
    "        scatter_params=dict(cmap=plt.cm.Spectral, c=X_color[idx_plot]),\n",
    "    )\n",
    "\n",
    "    plt.suptitle(rf\"Roseland embeddings for $landmarks={lm}$ in {dt:.2f} s.\", y=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic embedding selection\n",
    "\n",
    "In the visual comparison, we can identify good choices if the dimension is low (two dimensional for plotting). For larger (intrinsic) dimensions of the manifold, this becomes much harder or impossible. Furthermore, we also wish to automatize this process.\n",
    "\n",
    "In a (linear) Principal Component Analysis (PCA), the eigenvectors are sorted by relevance and each eigenvector points in the direction of (next) larger variance. This is a property by construction and because of this intrinsic order, the selection process is simpler as we only have to look at the magnitude of corresponding eigenvalues or a gap in the eigenvalues. \n",
    "\n",
    "For manifold learning models like `DiffusionMaps` or `Roseland` we trade-off lower-dimensional embeddings (by accounting for non-linearity) with a harder selection process. The eigenvectors with large eigenvalue may not add new information compared to the previous eigenvectors (as in the plot above). For an automatic selection of suitable eigenvector coordinates, we need a model that quantifies what a \"good\" embedding is and optimize for this quantity. \n",
    "\n",
    "An automatic selection model is provided in `datafold.dynfold.LocalRegressionSelection`. It provides two strategies (\"fixed dimension\" or \"threshold\"). Because we know *apriori* that the intrinsic dimension is two, we choose this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = LocalRegressionSelection(\n",
    "    intrinsic_dim=2, n_subsample=500, strategy=\"dim\"\n",
    ").fit(svdvecs)\n",
    "print(f\"Found parsimonious eigenvectors (indices): {selection.evec_indices_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the selection-model was able to find the same eigenvector pairs for embedding that we identified as the best choice before. Finally, we plot the unfolded point cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mapping = selection.transform(svdvecs)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(15, 9))\n",
    "ax.scatter(\n",
    "    target_mapping[idx_plot, 0],\n",
    "    target_mapping[idx_plot, 1],\n",
    "    c=X_color[idx_plot],\n",
    "    cmap=plt.cm.Spectral,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold learning on handwritten digits\n",
    "\n",
    "Disclaimer: Code parts are taken from [scikit-learn: Manifold learning on handwritten digits](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html). \n",
    "\n",
    "Based on the scikit-learn comparison of manifold learning models, we add the `Roseland` algorithm. We will also show the additional functionality of embedding unseen points (out-of-sampling). Also comapre to the 04_basic_dmap_digitclustering.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source code taken and adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html\n",
    "\n",
    "\n",
    "def plot_embedding(X, y, digits, title=None):\n",
    "    \"\"\"Scale and visualize the embedding vectors\"\"\"\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(\n",
    "            X[i, 0],\n",
    "            X[i, 1],\n",
    "            str(y[i]),\n",
    "            color=plt.cm.Set1(y[i] / 10.0),\n",
    "            fontdict={\"weight\": \"bold\", \"size\": 9},\n",
    "        )\n",
    "\n",
    "    if hasattr(offsetbox, \"AnnotationBbox\"):\n",
    "        # only print thumbnails with matplotlib > 1.0\n",
    "        shown_images = np.array([[1.0, 1.0]])  # just something big\n",
    "        for i in range(X.shape[0]):\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.r_[shown_images, [X[i]]]\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits[i], cmap=plt.cm.gray_r), X[i]\n",
    "            )\n",
    "            ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roseland embedding on the entire dataset\n",
    "\n",
    "We first carry out the same steps from the scikit-learn [comparison](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html) to load  the handwritten digits from the scikit-learn dataset and embed the entire available dataset. We optimize the parameters `epsilon` and `cut_off` using `PCManifold` (which uses the default `GaussianKernel`). These steps are optional, and the data `X` could have also just fitted directly with a user choice of `epsilon`. We use the default landmark set consisting of randomly selected 25% of the original dataset.\n",
    "\n",
    "We also illustrate the usage of the Roseland as a transfomrer in an `sklearn.pipeline`. The dimensionality of the dataset is first reduced using `PCA` before further manifold embedding by Roseland. Finally, to choose the svdvector coordinates for the embedding, we employ the `LocalRegressionSelection` as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "images = digits.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pcm = pfold.PCManifold(X)\n",
    "X_pcm.optimize_parameters(result_scaling=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = make_pipeline(\n",
    "    PCA(n_components=8),\n",
    "    dfold.Roseland(\n",
    "        n_svdtriplet=6,\n",
    "        kernel=pfold.GaussianKernel(\n",
    "            epsilon=X_pcm.kernel.epsilon, distance=dict(cut_off=X_pcm.cut_off)\n",
    "        ),\n",
    "        random_state=42,\n",
    "    ),\n",
    "    LocalRegressionSelection(intrinsic_dim=2, n_subsample=500, strategy=\"dim\"),\n",
    ")\n",
    "projection = transformer.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_embedding(\n",
    "    projection,\n",
    "    y,\n",
    "    images,\n",
    "    title=\"Roseland embedding of the digits\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}