{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings invariant to the observables with Mahalanobis kernel\n",
    "\n",
    "This short notebook demonstrates how to use the Mahalanobis distance with the predefined MahalanobisKernel in datafold.pcfold.kernels to obtain an embedding that is invariant to the observaton function. This idea and the example are taken from the following paper:\n",
    "\n",
    "**Reference:**\n",
    "\n",
    "Singer, A. & Coifman, R. R.: Non-linear independent component analysis with diffusion maps.\n",
    "Applied and Computational Harmonic Analysis, Elsevier BV, 2008, 25, 226-239. Available at: https://www.doi.org/10.1016/j.acha.2007.11.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "from datafold.pcfold.kernels import GaussianKernel, MahalanobisKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example data\n",
    "\n",
    "The standard example for this idea is the transformation of a square to a mushroom, through a transformation function that we assume is unknown in real examples. We only need access to neighbors of each point in the original space, pushed forward through the transformation. Actually, even less is needed: just the covariance matrix of these neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "n_pts = 2500\n",
    "n_neighbors = 100\n",
    "eps_covariance = 1e-2\n",
    "pinv_tol = np.exp(-11)\n",
    "\n",
    "rng = np.random.default_rng(random_state)\n",
    "\n",
    "\n",
    "def transformation(x):\n",
    "    return np.column_stack([x[:, 0] + x[:, 1] ** 3, x[:, 1] - x[:, 0] ** 3])\n",
    "\n",
    "\n",
    "# sample original data (not used until we compare at the end)\n",
    "data_x = rng.uniform(low=0, high=1, size=(n_pts, 2))\n",
    "# sample transformed data\n",
    "data_y_original = rng.uniform(low=0, high=1, size=(n_pts, 2))\n",
    "data_y = transformation(data_y_original)\n",
    "\n",
    "# sample covariance data from neighborhoods\n",
    "covariances = np.zeros((n_pts, 2, 2))\n",
    "for k in range(n_pts):\n",
    "    neighbors_x = rng.normal(\n",
    "        loc=data_y_original[k, :], scale=eps_covariance, size=(n_neighbors, 2)\n",
    "    )\n",
    "    neighbors_y = transformation(neighbors_x)\n",
    "    covariances[k, :, :] = np.cov(neighbors_y.T)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax[0].scatter(*data_x.T, s=5, c=data_x[:, 0])\n",
    "ax[0].scatter(*neighbors_x.T, s=5, c=\"red\")\n",
    "ax[0].set_xticks([0, 0.5, 1])\n",
    "ax[0].set_yticks([0, 0.5, 1])\n",
    "ax[0].set_xlabel(r\"$x_1$\")\n",
    "ax[0].set_ylabel(r\"$x_2$\")\n",
    "\n",
    "ax[1].scatter(*data_y.T, s=5, c=data_y_original[:, 0])\n",
    "ax[1].scatter(*neighbors_y.T, s=5, c=\"red\")\n",
    "ax[1].set_xlabel(r\"$y_1=x_1+x_2^3$\")\n",
    "ax[1].set_ylabel(r\"$y_2=x_2-x_1^3$\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the pseudo inverses of the covariances and pass them to the metric\n",
    "covariances_inv = np.zeros_like(covariances)\n",
    "\n",
    "some_evals1 = []\n",
    "some_ranks1 = []\n",
    "\n",
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "print(\"Computing %g inverse matrices...\" % (n_pts), end=\"\")\n",
    "for k in range(n_pts):\n",
    "    covariances_inv[k, :, :] = np.linalg.pinv(covariances[k, :, :], rcond=pinv_tol)\n",
    "    if k < 1000:\n",
    "        evals = np.linalg.eigvals(covariances[k, :, :])\n",
    "        some_evals1.append(evals)\n",
    "        some_ranks1.append(np.sum(evals > pinv_tol))\n",
    "print(f\"done in {time()-t0} seconds.\")\n",
    "\n",
    "some_evals1 = np.row_stack(some_evals1)\n",
    "some_ranks1 = np.row_stack(some_ranks1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can check if most covariance matrices have the correct rank (two). The pseudo-inverse computation should be set so that most matrices have the correct rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].hist(np.log(np.abs(some_evals1.ravel())), 150)\n",
    "ax[0].plot([np.log(pinv_tol), np.log(pinv_tol)], [0, 300], \"r-\", label=\"cutoff\")\n",
    "ax[0].set_title(\"covariance eigenvalue distribution\")\n",
    "ax[0].set_xlabel(r\"log $\\lambda$\")\n",
    "ax[0].legend()\n",
    "\n",
    "rank_bins = np.arange(0, 10) - 0.5\n",
    "ax[1].hist(some_ranks1, rank_bins - 0.05, alpha=0.5)\n",
    "ax[1].set_xlim([0, 5])\n",
    "ax[1].set_title(\"covariance rank distribution\")\n",
    "ax[1].set_xlabel(\"rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariant embedding\n",
    "\n",
    "Once the covariance matrices are obtained, we can use them to create an embedding of the mushroom data that is invariant to the observation function (here, the function from x to y). This will - in essence - create an embedding of the original square points x, even though we use the mushroom data y as an input.\n",
    "\n",
    "The MahalanobisKernel from datafold used in Diffusion maps internally constructs a sparse distance matrix and also automatically determines a good kernel bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute DMAPS with the given mahalanobis metric\n",
    "n_evecs = 10\n",
    "\n",
    "t0 = time()\n",
    "pcm = pfold.PCManifold(data_y)\n",
    "pcm.optimize_parameters(random_state=1, k=30, result_scaling=1)\n",
    "\n",
    "dmap = dfold.DiffusionMaps(\n",
    "    kernel=MahalanobisKernel(epsilon=8, distance={\"cut_off\": pcm.cut_off, \"k\": 30}),\n",
    "    n_eigenpairs=n_evecs,\n",
    ")\n",
    "dmap.fit(data_y, kernel_kwargs=dict(cov_matrices=covariances_inv))\n",
    "evecs1, evals1 = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "\n",
    "print(f\"Diffusion map done in {time()-t0} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(random_state)\n",
    "\n",
    "# Automatically select the embedding eigenfunctions\n",
    "lrs = dfold.LocalRegressionSelection(intrinsic_dim=2, n_subsample=500, strategy=\"dim\")\n",
    "selection1 = lrs.fit(evecs1)\n",
    "\n",
    "# plot the results\n",
    "idx_ev = rng.permutation(evecs1.shape[0])[0:2000]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax[0].plot(selection1.residuals_, \".-\")\n",
    "ax[0].plot(\n",
    "    [1, 2], selection1.residuals_[selection1.evec_indices_[:2]], \"r.\", label=\"selected\"\n",
    ")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(\n",
    "    evecs1[idx_ev, selection1.evec_indices_[0]],\n",
    "    evecs1[idx_ev, selection1.evec_indices_[1]],\n",
    "    s=2,\n",
    "    c=data_y_original[idx_ev, 0],\n",
    ")\n",
    "ax[1].set_title(\"invariant embedding\")\n",
    "ax[1].set_xlabel(r\"$\\phi_1$\")\n",
    "ax[1].set_ylabel(r\"$\\phi_2$\")\n",
    "ax[1].set_aspect(1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating similarity to original\n",
    "We can now easily demonstrate that the invariant embedding of the mushroom data y is the same embedding (up to isometry, here: rotation) as if we would have directly embedded the square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "pcm = pfold.PCManifold(data_x)\n",
    "pcm.optimize_parameters(random_state=1, k=30, result_scaling=1)\n",
    "dmap = dfold.DiffusionMaps(\n",
    "    kernel=GaussianKernel(epsilon=8, distance={\"cut_off\": pcm.cut_off, \"k\": 30}),\n",
    "    n_eigenpairs=n_evecs,\n",
    ")\n",
    "dmap.fit(pcm)\n",
    "evecs2, evals2 = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "print(f\"Diffusion map on x data done in {time()-t0} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection2 = lrs.fit(evecs2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6), sharey=True)\n",
    "\n",
    "ax[0].scatter(\n",
    "    evecs1[idx_ev, selection1.evec_indices_[0]],\n",
    "    evecs1[idx_ev, selection1.evec_indices_[1]],\n",
    "    s=2,\n",
    "    c=data_y_original[idx_ev, 0],\n",
    ")\n",
    "ax[0].set_title(\"embedding of y\")\n",
    "ax[0].set_xlabel(r\"$\\phi_1$\")\n",
    "ax[0].set_ylabel(r\"$\\phi_2$\")\n",
    "ax[0].set_aspect(1)\n",
    "\n",
    "ax[1].scatter(\n",
    "    evecs2[idx_ev, selection2.evec_indices_[0]],\n",
    "    evecs2[idx_ev, selection2.evec_indices_[1]],\n",
    "    s=2,\n",
    "    c=data_x[idx_ev, 0],\n",
    ")\n",
    "ax[1].set_title(\"embedding of x\")\n",
    "ax[1].set_xlabel(r\"$\\psi_1$\")\n",
    "ax[1].set_ylabel(r\"$\\psi_2$\")\n",
    "ax[1].set_aspect(1)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
