{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Visit the\n",
    "[documentation](https://datafold-dev.gitlab.io/datafold/tutorial_index.html) page\n",
    "to view the executed notebook.)\n",
    "\n",
    "# Geometric Harmonics: interpolate function values on data manifold\n",
    "\n",
    "Geometric harmonics are eigenvectors (corresponding to point evaluations of eigenfunctions) of a kernel matrix computed on data. For example, the kernel matrix from a Gaussian kernel\n",
    "\n",
    "$$W_{i,j} = \\exp{(-d^2_{i,j}/\\varepsilon)}, i,j = 1,\\ldots,M$$\n",
    "\n",
    "have geometric harmonics $\\phi$ and corresponding eigenvalues $\\lambda$ from, $(\\{\\phi_i\\}_{i=1}^{M}, \\{\\lambda\\}_{i=1}^{M} = \\operatorname{eig}(W)$. Performing interpolation with geometric harmonics builds up on the idea of the Nyström extension: instead of extending the geometric harmonics (i.e. eigenvectors) itself to a neighborhood region, the method allows to interpolate arbitrary function values defined on a manifold. \n",
    "\n",
    "The method is described in detail in\n",
    "\n",
    "*Coifman, Ronald R., and Stéphane Lafon. “Geometric Harmonics: A Novel Tool for Multiscale out-of-Sample Extension of Empirical Functions.” Applied and Computational Harmonic Analysis 21, no. 1 (July 2006): 31–52. https://doi.org/10.1016/j.acha.2005.07.005.*\n",
    "\n",
    "**Usecase: an out-of-sample method for Diffusion Map model**\n",
    "\n",
    "We set up a Diffusion Maps model that parametrizes an intrinsic low dimensional manifold $\\Psi$ in a high-dimensional dataset $X$. After we embedded the available data, we want to now learn the mapping\n",
    "\n",
    "$$f(X): X \\rightarrow \\Psi$$\n",
    "\n",
    "also for new samples $x_{new} \\notin X$ (image) or $\\psi \\notin \\Psi$ (pre-image). This is referred to as the so-called \"out-of-sample extension\". For image mappings, an out-of-sample method must be able to handle the high-dimensional ambient space of $ X $.\n",
    "\n",
    "The `DiffusionMaps` model maps the three-dimensional swiss-roll data $X$ to the two-dimensional manifold embedding $\\Psi$. We then learn an out-of-sample mapping $f$ and pre-image $f^{-1}$ between the two spaces with a geometric harmonics interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from skopt import gp_minimize\n",
    "from skopt.searchcv import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "\n",
    "import datafold.dynfold as dfold\n",
    "import datafold.pcfold as pfold\n",
    "from datafold.dynfold import (\n",
    "    GeometricHarmonicsInterpolator as GHI,\n",
    "    LocalRegressionSelection,\n",
    ")\n",
    "\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold embedding of swiss-roll data with diffusion maps\n",
    "\n",
    "We first generate the swiss-roll dataset with scikit-learn and then fit a `DiffusionMaps` model to compute the manifold embedding on the generated data. In this tutorial we skip the eigenvector selection process and choose the suitable embedding $\\Psi = \\{\\psi_1, \\psi_5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain dataset with a lot of points to get accurate eigenfunctions\n",
    "np.random.seed(random_state)\n",
    "X_all, color = make_swiss_roll(n_samples=15000, noise=0, random_state=random_state)\n",
    "\n",
    "num_eigenpairs = 6\n",
    "\n",
    "pcm = pfold.PCManifold(X_all)\n",
    "pcm.optimize_parameters(random_state=random_state)\n",
    "\n",
    "dmap = dfold.DiffusionMaps(\n",
    "    pfold.GaussianKernel(epsilon=pcm.kernel.epsilon),\n",
    "    n_eigenpairs=num_eigenpairs,\n",
    "    dist_kwargs=dict(cut_off=pcm.cut_off),\n",
    ")\n",
    "dmap = dmap.fit(pcm)\n",
    "evecs, evals = dmap.eigenvectors_, dmap.eigenvalues_\n",
    "\n",
    "# find the best eigenvectors automatically\n",
    "selection = LocalRegressionSelection(\n",
    "    intrinsic_dim=2, n_subsample=500, strategy=\"dim\"\n",
    ").fit(dmap.eigenvectors_)\n",
    "\n",
    "psi_all = evecs[:, selection.evec_indices_]\n",
    "\n",
    "# select a subset of the data to proceed, so that the next computations are less expensive\n",
    "ind_subset = np.sort(np.random.permutation(np.arange(X_all.shape[0]))[0:2500])\n",
    "X_all = X_all[ind_subset, :]\n",
    "psi_all = psi_all[ind_subset, :]\n",
    "color = color[ind_subset]\n",
    "\n",
    "# Plotting\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "fig.suptitle(f\"total #samples={pcm.shape[0]}\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "ax.set_title(\"all data, original coordinates\")\n",
    "ax.scatter(*X_all.T, s=5, c=color, cmap=plt.cm.Spectral)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.set_title(\"all data, embedding coordinates\")\n",
    "ax.scatter(*psi_all.T, s=5, c=color, cmap=plt.cm.Spectral)\n",
    "ax.set_xlabel(r\"$\\psi_\" + str(selection.evec_indices_[0]) + r\"$\")\n",
    "ax.set_ylabel(r\"$\\psi_\" + str(selection.evec_indices_[1]) + r\"$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image mapping\n",
    "\n",
    "We now have both the original dataset $X$ (=`X_all`) and the embedding $\\Psi$ (=`psi_all`). We proceed to train the `GeometricHarmonicsInterpolator` model, which will perform the out-of-sample mapping. For the image mapping, we want to learn the function $f: X \\rightarrow \\Psi$. Here, the ambient space of $X$ is relatively low with only three dimensions, which is mainly for plotting purposes. If the same data manifold was embedded in a much larger space, say 100 by linearly transforming it into this space, we would obtain the same results.\n",
    "\n",
    "We split the datasets $X$ and $\\Psi$ into a training and test set accordingly. We do not use the test set for parameter optimization, but later to compare the out-of-sample for \"new\" points against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the psi_test values are used for the \"ground truth\" values to measure an error\n",
    "# the color values are used for plotting\n",
    "X_train, X_test, psi_train, psi_test, color_train, color_test = train_test_split(\n",
    "    X_all, psi_all, color, train_size=2 / 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GeometricHarmonicsInterpolator` has two important parameters, the number of geometric harmonics (`n_eigenpairs`) and the kernel scale (`epsilon`). We manually select the two values and refer to the end of this tutorial where we also use Bayesian optimization to find suitable parameters in the pre-image problem. \n",
    "\n",
    "Note that the `DiffusionMaps` model also comes with a native image mapping, based on the Nyström extension. A direct comparison, however, is difficult, because here we use the `DiffusionMap` embedding as ground truth and cannot map truly new samples. \n",
    "\n",
    "**Note**: The eigenvalues corresponding to the geometric harmonics (kernel eigenvectors) are $\\lambda_i \\rightarrow 0, \\text{ for } i \\rightarrow \\infty$. The geometric harmonics interpolation (like the Nyström extension) involves the reciprocal of eigenvalues $1/\\lambda_i$. This means, that too many eigenpairs can lead to numerical instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the geometric harmonics from X to \\Psi\n",
    "n_eigenpairs = 100\n",
    "epsilon = 30\n",
    "\n",
    "# construct the GeometricHarmonicsInterpolator and fit it to the data.\n",
    "gh_interpolant = GHI(\n",
    "    pfold.GaussianKernel(epsilon=epsilon),\n",
    "    n_eigenpairs=n_eigenpairs,\n",
    "    dist_kwargs=dict(cut_off=np.inf),\n",
    ")\n",
    "\n",
    "gh_interpolant.fit(X_train, psi_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate interpolation function for image\n",
    "\n",
    "Because `GeometricHarmonicsInterpolator` follows the scikit-learn API, we can now use the `.score` method to evaluate the residual and the error for the validation points. The default metric is a mean squared error. \n",
    "\n",
    "**Note** that the model scores are the mean squared error, we therefore maximize negative errors. This is according to the definition of scikit-learn's scoring parameter which [states](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) \"higher return values are better than lower return values\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train = gh_interpolant.score(X_train, psi_train)\n",
    "error_test = gh_interpolant.score(X_test, psi_test)\n",
    "\n",
    "# use pandas for table\n",
    "df = pd.DataFrame(\n",
    "    np.row_stack([residual_train, error_test]),\n",
    "    index=[\"residual\", \"error\"],\n",
    "    columns=[\"psi1\", \"psi5\"],\n",
    ")\n",
    "\n",
    "print(f\"mean: \\n{df.mean()}\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left side, we plot the ground truth test set embedding (from the `DiffusionMaps` model). On the right side, we predict the embedding values with geometric harmonics interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[12, 5], sharey=True)\n",
    "\n",
    "ax[0].set_title(\"ground truth test set ${\\psi}_{1,5}$\")\n",
    "ax[0].scatter(psi_test[:, 0], psi_test[:, 1], s=10, c=color_test, cmap=plt.cm.Spectral)\n",
    "ax[0].set_xlabel(r\"$\\psi_1$\")\n",
    "ax[0].set_ylabel(r\"$\\psi_5$\")\n",
    "\n",
    "ax[1].scatter(\n",
    "    *gh_interpolant.predict(X_test).T, s=10, c=color_test, cmap=plt.cm.Spectral\n",
    ")\n",
    "ax[1].set_title(r\"interpolated values $\\hat{\\psi}_{1,5}$\")\n",
    "ax[1].set_xlabel(r\"$\\hat{\\psi}_1$\")\n",
    "ax[1].set_ylabel(r\"$\\hat{\\psi}_5$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_test_interp = gh_interpolant.predict(X_test)\n",
    "\n",
    "error_color = (psi_test[:, 0] - psi_test_interp[:, 0]) ** 2 + (\n",
    "    psi_test[:, 1] - psi_test_interp[:, 1]\n",
    ") ** 2\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=[12, 5], sharey=True)\n",
    "\n",
    "sc = ax[0].scatter(psi_test[:, 0], psi_test[:, 1], c=error_color, cmap=\"Reds\")\n",
    "ax[0].set_title(\"test data with error (absolute)\")\n",
    "plt.colorbar(sc, ax=ax[0])\n",
    "ax[0].set_xlabel(r\"${\\psi}_1$\")\n",
    "ax[0].set_ylabel(r\"${\\psi}_5$\")\n",
    "\n",
    "# the np.newaxis need are required to have 2D arrays:\n",
    "norm_factor = np.max(\n",
    "    [\n",
    "        np.max(pdist(psi_test[0, :][:, np.newaxis])),\n",
    "        np.max(pdist(psi_test[1, :][:, np.newaxis])),\n",
    "    ]\n",
    ")  # take max. distance in test dataset as the norming factor\n",
    "\n",
    "sc = ax[1].scatter(\n",
    "    psi_test[:, 0],\n",
    "    psi_test[:, 1],\n",
    "    vmin=0,\n",
    "    vmax=0.1,\n",
    "    c=error_color / norm_factor,\n",
    "    cmap=\"Reds\",\n",
    ")\n",
    "plt.colorbar(sc, ax=ax[1])\n",
    "ax[1].set_title(f\"test data with error (relative)\")\n",
    "ax[1].set_xlabel(r\"${\\psi}_1$\")\n",
    "ax[1].set_ylabel(r\"${\\psi}_5$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Image mapping\n",
    "\n",
    "We now want to learn the inverse mapping and interpolate the $X$ values (as function values) on $\\Psi$.\n",
    "\n",
    "$$f^{-1}: \\Psi \\rightarrow X$$\n",
    " \n",
    "In the literature, this is often referred to as the pre-image problem of manifold learning.\n",
    "\n",
    "We repeat the above steps and use the same embedding (but re-shuffle the training and test set). \n",
    "\n",
    "In the following, we will learn a `GeometricHarmonicsInterpolator` model\n",
    "\n",
    "1. optimize the model parameters with `PCManifold.optimize_parameters()`\n",
    "2. use `BayesSearchCV` from the `skopt` package to optimize for model parameters in a search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "# NOTE: this time, the \"ground truth\" values are X_test (-> target values)\n",
    "\n",
    "# shuffle new training and test set\n",
    "X_train, X_test, psi_train, psi_test, color_train, color_test = train_test_split(\n",
    "    X_all, psi_all, color, train_size=2 / 3, random_state=random_state\n",
    ")\n",
    "\n",
    "# (!!) we use the DMAP space as base space now, and interpolate X as function values\n",
    "pcm = pfold.PCManifold(psi_train)\n",
    "pcm.optimize_parameters(random_state=random_state)\n",
    "\n",
    "opt_epsilon = pcm.kernel.epsilon\n",
    "opt_cutoff = pcm.cut_off\n",
    "opt_n_eigenpairs = 100\n",
    "\n",
    "# test the interpolation quality with PCManifold optimization\n",
    "gh_interpolant_psi_to_X = GHI(\n",
    "    pfold.GaussianKernel(epsilon=opt_epsilon),\n",
    "    n_eigenpairs=opt_n_eigenpairs,\n",
    "    dist_kwargs=dict(cut_off=opt_cutoff),\n",
    ")\n",
    "\n",
    "\n",
    "gh_interpolant_psi_to_X.fit(psi_train, X_train)\n",
    "\n",
    "# compute residual and error\n",
    "residual = gh_interpolant_psi_to_X.score(psi_train, X_train)\n",
    "error = gh_interpolant_psi_to_X.score(psi_test, X_test)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.row_stack([residual, error]),\n",
    "    index=[\"residual\", \"error\"],\n",
    "    columns=[\"x\", \"y\", \"z\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the error and residual is high and confirm this with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "# plot ground truth and interpolated values\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"ground truth values\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    *(gh_interpolant_psi_to_X.predict(psi_test)).T, c=color_test, cmap=plt.cm.Spectral\n",
    ")\n",
    "ax.set_title(\"interpolated values (no direct hyperparameter optimization)\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the `BayesSearchCV` (Bayesian optimization) model using Gaussian processes from the Python package `skopt`.\n",
    "\n",
    "That cross-validation is tricky with kernel methods: Reducing the number of samples also changes the optimal parameters for the entire dataset. We, therefore, refit the model only on the training data and not on all the available data (training and test). \n",
    "\n",
    "\n",
    "Because the `BayesSearchCV` only allows to optimize numeric values (`Real` and `Integer`) values, we define a subclass of `GeometricHarmonicsInterpolator` that sets internally always a `GaussianKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "n_iters = 5\n",
    "np.random.seed(random_state)\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.random.permutation(X_train.shape[0]), train_size=2 / 3, test_size=1 / 3\n",
    ")\n",
    "\n",
    "\n",
    "class GHIGauss(GHI):\n",
    "    def __init__(self, epsilon=1, n_eigenpairs=2, cut_off=np.inf):\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.n_eigenpairs = n_eigenpairs\n",
    "        self.cut_off = cut_off\n",
    "\n",
    "        super(GHIGauss, self).__init__(\n",
    "            kernel=pfold.GaussianKernel(self.epsilon),\n",
    "            n_eigenpairs=self.n_eigenpairs,\n",
    "            is_stochastic=False,\n",
    "            dist_kwargs=dict(cut_off=self.cut_off),\n",
    "        )\n",
    "\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    GHIGauss(),\n",
    "    {\n",
    "        \"epsilon\": Real(\n",
    "            pcm.kernel.epsilon / 2, pcm.kernel.epsilon * 2, prior=\"log-uniform\"\n",
    "        ),\n",
    "        \"cut_off\": Real(pcm.cut_off / 2, pcm.cut_off * 2, prior=\"uniform\"),\n",
    "        \"n_eigenpairs\": Integer(10, 1000, prior=\"uniform\"),\n",
    "    },\n",
    "    n_iter=n_iters,\n",
    "    random_state=0,\n",
    "    scoring=lambda estimator, x, y: estimator.score(\n",
    "        x, y, multioutput=\"uniform_average\"\n",
    "    ),  # is to be maximized\n",
    "    cv=[[train_indices, test_indices]],\n",
    "    refit=False,  # we cannot refit to the entire dataset because this would alter the optimal kernel scale\n",
    ")\n",
    "\n",
    "# run the Bayesian optimization\n",
    "opt.fit(psi_train, X_train)\n",
    "\n",
    "# get best model and results from parameter search\n",
    "\n",
    "# refit best parameter set on training set (not entire dataset - the parameters are optimized for the training set!)\n",
    "optimal_GHI = GHIGauss(**opt.best_params_).fit(\n",
    "    psi_train[train_indices, :], X_train[train_indices, :]\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Previous epsilon: {pcm.kernel.epsilon}, cut-off: {pcm.cut_off}, #eigenpairs: {num_eigenpairs}\"\n",
    ")\n",
    "print(\n",
    "    f\"Optimal epsilon: {optimal_GHI.epsilon}, cut-off: {optimal_GHI.cut_off}, #eigenpairs: {optimal_GHI.n_eigenpairs}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate interpolation function for pre-image\n",
    "\n",
    "We check and visualize the results with the parameters obtained from the Bayesian optimization search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "# compute residual and error for best model\n",
    "residual = optimal_GHI.score(psi_train, X_train)\n",
    "error = optimal_GHI.score(psi_test, X_test)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.row_stack([residual, error]),\n",
    "    index=[\"residual\", \"error\"],\n",
    "    columns=[\"x\", \"y\", \"z\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "# plot information about the geometric harmonics (i.e. eigenvalues and eigenvectors)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.semilogy(optimal_GHI.eigenvalues_, \".-\")\n",
    "ax.set_xlabel(\"#eigenvalue\")\n",
    "ax.set_ylabel(r\"$\\lambda$\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 6, figsize=[10, 3], sharex=True, sharey=True)\n",
    "for k1 in range(ax.shape[0]):\n",
    "    for k2 in range(ax.shape[1]):\n",
    "        index = k2 + k1 * ax.shape[1]\n",
    "        ax[k1, k2].scatter(\n",
    "            *psi_train[train_indices, :].T,\n",
    "            s=10,\n",
    "            c=optimal_GHI.eigenvectors_[:, index],\n",
    "            cmap=plt.cm.Spectral,\n",
    "        )\n",
    "        ax[k1, k2].set_title(r\"$\\psi_{\" + str(index) + \"}$\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "# plot ground truth and interpolated values\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"ground truth values\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "ax.scatter(*(optimal_GHI.predict(psi_test)).T, c=color_test, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"interpolated values\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# (disabled because scikit-optimize is not compatible with the current sklearn>=0.24.1)\n",
    "\n",
    "# compute and plot error\n",
    "\n",
    "error_color = np.linalg.norm(X_test - optimal_GHI.predict(psi_test), axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=[12, 5])\n",
    "\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "sc = ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=error_color, cmap=\"Reds\")\n",
    "plt.title(\"test data with error (absolute)\")\n",
    "plt.colorbar(sc)\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection=\"3d\")\n",
    "# the np.newaxis need are required to have 2D arrays:\n",
    "norm_factor = np.max(\n",
    "    [\n",
    "        np.max(pdist(X_test[0, :][:, np.newaxis])),\n",
    "        np.max(pdist(X_test[1, :][:, np.newaxis])),\n",
    "        np.max(pdist(X_test[2, :][:, np.newaxis])),\n",
    "    ]\n",
    ")  # take max. distance in test dataset as the norming factor\n",
    "\n",
    "sc = ax.scatter(\n",
    "    X_test[:, 0],\n",
    "    X_test[:, 1],\n",
    "    X_test[:, 2],\n",
    "    vmin=0,\n",
    "    vmax=0.1,\n",
    "    c=error_color / norm_factor,\n",
    "    cmap=\"Reds\",\n",
    ")\n",
    "plt.colorbar(sc)\n",
    "plt.title(f\"test data with error (relative)\")\n",
    "ax.set_xlabel(r\"$x_1$\")\n",
    "ax.set_ylabel(r\"$x_2$\")\n",
    "ax.set_zlabel(r\"$x_3$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
