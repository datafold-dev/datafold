
@article{le_clainche_higher_2017,
	title = {Higher order dynamic mode decomposition of noisy experimental data: {The} flow structure of a zero-net-mass-flux jet},
	volume = {88},
	issn = {08941777},
	shorttitle = {Higher order dynamic mode decomposition of noisy experimental data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089417771730184X},
	doi = {10.1016/j.expthermflusci.2017.06.011},
	abstract = {A method is presented to treat complex experimental flow data resulting from PIV. The method is based on an appropriate combination of higher order singular value decomposition (which cleans the data along the temporal dimension and the various space dimensions) and higher order dynamic mode decomposition (HODMD), a recent extension of standard dynamic mode decomposition that treats the data in a sliding window. The performance of the method is tested using experimental data obtained in the near field of a zero-net-mass-flux (ZNMF) jet. The better performance of HODMD is put in evidence making this technique suitable to both, cleaning the experimental noise using a limited number of snapshots and obtaining robust and sufficiently accurate results that elucidate the spatio-temporal structure of the flow. The results show that this ZNMF jet is temporally periodic in the near field, where the flow results from the interaction of a large number harmonics. These harmonics involve large scale spatial flow structures, identified as spatially growing instabilities, which are associated with the flow transition to turbulence in the far field.},
	language = {en},
	journal = {Experimental Thermal and Fluid Science},
	author = {Le Clainche, Soledad and Vega, Jos{\'e} M. and Soria, Julio},
	month = nov,
	year = {2017},
	pages = {336--353},
}

@article{berry_variable_2016,
	title = {Variable bandwidth diffusion kernels},
	volume = {40},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520315000020},
	doi = {10.1016/j.acha.2015.01.001},
	abstract = {Practical applications of kernel methods often use variable bandwidth kernels, also known as self-tuning kernels, however much of the current theory of kernel based techniques is only applicable to fixed bandwidth kernels. In this paper, we derive the asymptotic expansion of these variable bandwidth kernels for arbitrary bandwidth functions; generalizing the theory of Diffusion Maps and Laplacian Eigenmaps. We also derive pointwise error estimates for the corresponding discrete operators which are based on finite data sets; generalizing a result of Singer which was restricted to fixed bandwidth kernels. Our analysis reveals how areas of small sampling density lead to large errors, particularly for fixed bandwidth kernels. We explain the limitation of the existing theory to data sampled from compact manifolds by showing that when the sampling density is not bounded away from zero (which implies that the data lies on an open set) the error estimates for fixed bandwidth kernels will be unbounded. We show that this limitation can be overcome by choosing a bandwidth function inversely proportional to the sampling density (which can be estimated from data) which allows us to control the error estimates uniformly over a noncompact manifold. We numerically verify these results on non-compact manifolds by constructing the generator of the Ornstein{\textendash}Uhlenbeck process on a real line and a two-dimensional plane using data sampled independently from the respective invariant measures. We also verify our results on compact manifolds by constructing the Laplacian on the unit circle and the unit sphere and we show that the variable bandwidth kernels exhibit reduced sensitivity to bandwidth selection and give better results for an automatic bandwidth selection algorithm.},
	language = {en},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Berry, Tyrus and Harlim, John},
	month = jan,
	year = {2016},
	pages = {68--96},
}

@article{dsilva_parsimonious_2018,
	title = {Parsimonious representation of nonlinear dynamical systems through manifold learning: {A} chemotaxis case study},
	volume = {44},
	issn = {10635203},
	shorttitle = {Parsimonious representation of nonlinear dynamical systems through manifold learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520315000949},
	doi = {10.1016/j.acha.2015.06.008},
	abstract = {Nonlinear manifold learning algorithms, such as diffusion maps, have been fruitfully applied in recent years to the analysis of large and complex data sets. However, such algorithms still encounter challenges when faced with real data. One such challenge is the existence of {\textquotedblleft}repeated eigendirections,{\textquotedblright} which obscures the detection of the true dimensionality of the underlying manifold and arises when several embedding coordinates parametrize the same direction in the intrinsic geometry of the data set. We propose an algorithm, based on local linear regression, to automatically detect coordinates corresponding to repeated eigendirections. We construct a more parsimonious embedding using only the eigenvectors corresponding to unique eigendirections, and we show that this reduced diffusion maps embedding induces a metric which is equivalent to the standard diffusion distance. We first demonstrate the utility and flexibility of our approach on synthetic data sets. We then apply our algorithm to data collected from a stochastic model of cellular chemotaxis, where our approach for factoring out repeated eigendirections allows us to detect changes in dynamical behavior and the underlying intrinsic system dimensionality directly from data.},
	language = {en},
	number = {3},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Dsilva, Carmeline J. and Talmon, Ronen and Coifman, Ronald R. and Kevrekidis, Ioannis G.},
	month = may,
	year = {2018},
	pages = {759--773},
}

@article{champion_discovery_2019,
	title = {Discovery of {Nonlinear} {Multiscale} {Systems}: {Sampling} {Strategies} and {Embeddings}},
	volume = {18},
	issn = {1536-0040},
	shorttitle = {Discovery of {Nonlinear} {Multiscale} {Systems}},
	url = {https://epubs.siam.org/doi/10.1137/18M1188227},
	doi = {10.1137/18M1188227},
	abstract = {A major challenge in the study of dynamical systems is that of model discovery: turning data into models that are not just predictive, but provide insight into the nature of the underlying dynamical system that generated the data. This problem is made more difficult by the fact that many systems of interest exhibit diverse behaviors across multiple time scales. We introduce a number of datadriven strategies for discovering nonlinear multiscale dynamical systems and their embeddings from data. We consider two canonical cases: (i) systems for which we have full measurements of the governing variables and (ii) systems for which we have incomplete measurements. For systems with full state measurements, we show that the recent sparse identification of nonlinear dynamical systems (SINDy) method can discover governing equations with relatively little data, provided that accurate measurements of the derivatives can be computed from the data. We introduce a sampling method that allows SINDy to scale efficiently to problems with multiple time scales; specifically, we can discover distinct governing equations at slow and fast scales. For systems with incomplete observations, we show that the Hankel alternative view of Koopman (HAVOK) method, based on time-delay embedding coordinates, can be used to obtain a linear model and Koopman invariant measurement system that nearly perfectly captures the dynamics of nonlinear quasiperiodic systems on the attractor. We introduce two strategies for using HAVOK on systems with multiple time scales. Together, our approaches provide a suite of mathematical strategies for reducing the data required to discover and model nonlinear multiscale systems.},
	language = {en},
	number = {1},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Champion, Kathleen P. and Brunton, Steven L. and Kutz, J. Nathan},
	month = jan,
	year = {2019},
	pages = {312--333},
}

@article{tu_dynamic_2014,
	title = {On {Dynamic} {Mode} {Decomposition}: {Theory} and {Applications}},
	volume = {1},
	issn = {2158-2491},
	shorttitle = {On {Dynamic} {Mode} {Decomposition}},
	url = {http://arxiv.org/abs/1312.0041},
	doi = {10.3934/jcd.2014.1.391},
	abstract = {Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken. We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator. This generalizes DMD to a larger class of datasets, including nonsequential time series. We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational efficiency and mitigate the effects of noise, respectively. We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples. Such computations are not considered in the existing literature, but can be understood using our more general framework. In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory. It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science. We show that under certain conditions, DMD is equivalent to LIM.},
	language = {en},
	number = {2},
	journal = {Journal of Computational Dynamics},
	author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
	month = dec,
	year = {2014},
	note = {arXiv: 1312.0041},
	keywords = {koopman},
	pages = {391--421},
}

@article{williams_datadriven_2015,
	title = {A {Data}{\textendash}{Driven} {Approximation} of the {Koopman} {Operator}: {Extending} {Dynamic} {Mode} {Decomposition}},
	volume = {25},
	issn = {0938-8974, 1432-1467},
	shorttitle = {A {Data}{\textendash}{Driven} {Approximation} of the {Koopman} {Operator}},
	url = {http://link.springer.com/10.1007/s00332-015-9258-5},
	doi = {10.1007/s00332-015-9258-5},
	language = {en},
	number = {6},
	journal = {Journal of Nonlinear Science},
	author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
	month = dec,
	year = {2015},
	keywords = {EDMD},
	pages = {1307--1346},
}

@article{arbabi_ergodic_2017,
	title = {Ergodic {Theory}, {Dynamic} {Mode} {Decomposition}, and {Computation} of {Spectral} {Properties} of the {Koopman} {Operator}},
	volume = {16},
	issn = {1536-0040},
	url = {https://epubs.siam.org/doi/10.1137/17M1125236},
	doi = {10.1137/17M1125236},
	abstract = {We establish the convergence of a class of numerical algorithms, known as dynamic mode decomposition (DMD), for computation of the eigenvalues and eigenfunctions of the infinite-dimensional Koopman operator. The algorithms act on data coming from observables on a state space, arranged in Hankel-type matrices. The proofs utilize the assumption that the underlying dynamical system is ergodic. This includes the classical measure-preserving systems, as well as systems whose attractors support a physical measure. Our approach relies on the observation that vector projections in DMD can be used to approximate the function projections by the virtue of Birkhoff{\textquoteright}s ergodic theorem. Using this fact, we show that applying DMD to Hankel data matrices in the limit of infinite-time observations yields the true Koopman eigenfunctions and eigenvalues. We also show that the singular value decomposition, which is the central part of most DMD algorithms, converges to the proper orthogonal decomposition of observables. We use this result to obtain a representation of the dynamics of systems with continuous spectrum based on the lifting of the coordinates to the space of observables. The numerical application of these methods is demonstrated using well-known dynamical systems and examples from computational fluid dynamics.},
	language = {en},
	number = {4},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Arbabi, Hassan and Mezi{\'c}, Igor},
	month = jan,
	year = {2017},
	pages = {2096--2126},
}

@phdthesis{lafon_diffusion_2004,
	title = {Diffusion {Maps} and {Geometric} {Harmonics}},
	language = {en},
	school = {Yale University},
	author = {Lafon, St{\'e}phane S},
	year = {2004},
}

@article{giannakis_data-driven_2019,
	title = {Data-driven spectral decomposition and forecasting of ergodic dynamical systems},
	volume = {47},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520317300982},
	doi = {10.1016/j.acha.2017.09.001},
	abstract = {We develop a framework for dimension reduction, mode decomposition, and nonparametric forecasting of data generated by ergodic dynamical systems. This framework is based on a representation of the Koopman and Perron{\textendash}Frobenius groups of unitary operators in a smooth orthonormal basis of the L2 space of the dynamical system, acquired from time-ordered data through the diffusion maps algorithm. Using this representation, we compute Koopman eigenfunctions through a regularized advection{\textendash}diffusion operator, and employ these eigenfunctions in dimension reduction maps with projectible dynamics and high smoothness for the given observation modality. In systems with pure point spectra, we construct a decomposition of the generator of the Koopman group into mutually commuting vector fields that transform naturally under changes of observation modality, which we reconstruct in data space through a representation of the pushforward map in the Koopman eigenfunction basis. We also establish a correspondence between Koopman operators and Laplace{\textendash}Beltrami operators constructed from data in Takens delaycoordinate space, and use this correspondence to provide an interpretation of diffusion-mapped delay coordinates for this class of systems. Moreover, we take advantage of a special property of the Koopman eigenfunction basis, namely that the basis elements evolve as simple harmonic oscillators, to build nonparametric forecast models for probability densities and observables. In systems with more complex spectral behavior, including mixing systems, we develop a method inspired from time change in dynamical systems to transform the generator to a new operator with potentially improved spectral properties, and use that operator for vector field decomposition and nonparametric forecasting.},
	language = {en},
	number = {2},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Giannakis, Dimitrios},
	month = sep,
	year = {2019},
	keywords = {Koopman, Diffusion Maps},
	pages = {338--396},
}

@article{berry_time-scale_2013,
	title = {Time-{Scale} {Separation} from {Diffusion}-{Mapped} {Delay} {Coordinates}},
	volume = {12},
	issn = {1536-0040},
	url = {http://epubs.siam.org/doi/10.1137/12088183X},
	doi = {10.1137/12088183X},
	abstract = {It has long been known that the method of time-delay embedding can be used to reconstruct nonlinear dynamics from time series data. A less-appreciated fact is that the induced geometry of time-delay coordinates increasingly biases the reconstruction toward the stable directions as delays are added. This bias can be exploited, using the diffusion maps approach to dimension reduction, to extract dynamics on desired time scales from high-dimensional observed data. We demonstrate the technique on a wide range of examples, including data generated by a model of meandering spiral waves and video recordings of a liquid-crystal experiment.},
	language = {en},
	number = {2},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Berry, T. and Cressman, J. R. and Greguri{\'c}-Feren{\v c}ek, Z. and Sauer, T.},
	month = jan,
	year = {2013},
	pages = {618--649},
}

@article{fernandez_diffusion_2015,
	title = {Diffusion {Maps} for dimensionality reduction and visualization of meteorological data},
	volume = {163},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231215004257},
	doi = {10.1016/j.neucom.2014.08.090},
	abstract = {The growing interest in big data problems implies the need for unsupervised methods for data visualization and dimensionality reduction. Diffusion Maps (DM) is a recent technique that can capture the lower dimensional geometric structure underlying the sample patterns in a way which can be made to be independent of the sampling distribution. Moreover, DM allows to define an embedding whose Euclidean metric relates to the sample{\textquoteright}s intrinsic one which, in turn, enables a principled application of k-means clustering. In this work we give a self-contained review of DM and discuss two methods to compute the DM embedding coordinates to new out-of-sample data. Then, we will apply them on two meteorological data problems that involve respectively time and spatial compression of numerical weather forecasts and show how DM is capable to, first, greatly reduce the initial dimension while still capturing relevant information in the original data and, also, how the sample-derived DM embedding coordinates can be extended to new patterns.},
	language = {en},
	journal = {Neurocomputing},
	author = {Fern{\'a}ndez, {\'A}ngela and Gonz{\'a}lez, Ana M. and D{\'i}az, Julia and Dorronsoro, Jos{\'e} R.},
	month = sep,
	year = {2015},
	pages = {25--37},
}

@article{chiavazzo_reduced_2014,
	title = {Reduced {Models} in {Chemical} {Kinetics} via {Nonlinear} {Data}-{Mining}},
	volume = {2},
	issn = {2227-9717},
	url = {http://www.mdpi.com/2227-9717/2/1/112},
	doi = {10.3390/pr2010112},
	language = {en},
	number = {1},
	journal = {Processes},
	author = {Chiavazzo, Eliodoro and Gear, Charles and Dsilva, Carmeline and Rabin, Neta and Kevrekidis, Ioannis},
	month = jan,
	year = {2014},
	pages = {112--140},
}

@inproceedings{rabin_heterogeneous_2012,
	title = {Heterogeneous datasets representation and learning using diffusion maps and {Laplacian} pyramids},
	isbn = {978-1-61197-232-0 978-1-61197-282-5},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972825.17},
	doi = {10.1137/1.9781611972825.17},
	abstract = {The diffusion maps together with the geometric harmonics provide a method for describing the geometry of high dimensional data and for extending these descriptions to new data points and to functions, which are defined on the data. This method suffers from two limitations. First, even though real-life data is often heterogeneous , the assumption in diffusion maps is that the attributes of the processed dataset are comparable. Second, application of the geometric harmonics requires careful setting for the correct extension scale and condition number. In this paper, we propose a method for representing and learning heterogeneous datasets by using diffusion maps for unifying and embedding heterogeneous dataset and by replacing the geometric harmonics with the Laplacian pyramid extension. Experimental results on three benchmark datasets demonstrate how the learning process becomes straightforward when the constructed representation smoothly parameterizes the task-related function.},
	language = {en},
	booktitle = {Proceedings of the 2012 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Rabin, Neta and Coifman, Ronald R.},
	month = apr,
	year = {2012},
	pages = {189--199},
}

@article{coifman_geometric_2006,
	title = {Geometric harmonics: {A} novel tool for multiscale out-of-sample extension of empirical functions},
	volume = {21},
	issn = {10635203},
	shorttitle = {Geometric harmonics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000522},
	doi = {10.1016/j.acha.2005.07.005},
	abstract = {We describe a simple scheme, based on the Nystr{\"o}m method, for extending empirical functions f defined on a set X to a larger set X{\textasciimacron} . The extension process that we describe involves the construction of a specific family of functions that we term geometric harmonics. These functions constitute a generalization of the prolate spheroidal wave functions of Slepian in the sense that they are optimally concentrated on X. We study the case when X is a submanifold of Rn in greater detail. In this situation, any empirical function f on X can be characterized by its decomposition over the intrinsic Fourier modes, i.e., the eigenfunctions of the Laplace{\textendash}Beltrami operator, and we show that this intrinsic frequency spectrum determines the largest domain of extension of f to the entire space Rn. Our analysis relates the complexity of the function on the training set to the scale of extension off this set. This approach allows us to present a novel multiscale extension scheme for empirical functions.},
	language = {en},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
	month = jul,
	year = {2006},
	pages = {31--52},
}

@article{coifman_diffusion_2006,
	title = {Diffusion maps},
	volume = {21},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
	doi = {10.1016/j.acha.2006.04.006},
	abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.},
	language = {en},
	number = {1},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
	month = jul,
	year = {2006},
	pages = {5--30},
}

@article{bengio_learning_2004,
	title = {Learning {Eigenfunctions} {Links} {Spectral} {Embedding} and {Kernel} {PCA}},
	volume = {16},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/0899766041732396},
	doi = {10.1162/0899766041732396},
	abstract = {In this paper, we show a direct relation between spectral embedding methods and kernel PCA, and how both are special cases of a more general learning problem, that of learning the principal eigenfunctions of an operator defined from a kernel and the unknown data generating density.},
	language = {en},
	number = {10},
	journal = {Neural Computation},
	author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le and Paiement, Jean-Fran{\c c}ois and Vincent, Pascal and Ouimet, Marie},
	month = oct,
	year = {2004},
	pages = {2197--2219},
}

@article{fernandez_auto-adaptative_2014,
	title = {Auto-adaptative {Laplacian} {Pyramids} for {High}-dimensional {Data} {Analysis}},
	url = {http://arxiv.org/abs/1311.6594},
	abstract = {Non-linear dimensionality reduction techniques such as manifold learning algorithms have become a common way for processing and analyzing high-dimensional patterns that often have attached a target that corresponds to the value of an unknown function. Their application to new points consists in two steps: first, embedding the new data point into the low dimensional space and then, estimating the function value on the test point from its neighbors in the embedded space.},
	language = {en},
	journal = {arXiv:1311.6594 [cs, stat]},
	author = {Fern{\'a}ndez, {\'A}ngela and Rabin, Neta and Fishelov, Dalia and Dorronsoro, Jos{\'e} R.},
	month = may,
	year = {2014},
	note = {arXiv: 1311.6594},
}

@article{berry_consistent_2019,
	title = {Consistent {Manifold} {Representation} for {Topological} {Data} {Analysis}},
	url = {http://arxiv.org/abs/1606.02353},
	abstract = {For data sampled from an arbitrary density on a manifold embedded in Euclidean space, the Continuous k-Nearest Neighbors (CkNN) graph construction is introduced. It is shown that CkNN is geometrically consistent in the sense that under certain conditions, the unnormalized graph Laplacian converges to the Laplace-Beltrami operator, spectrally as well as pointwise. It is proved for compact (and conjectured for noncompact) manifolds that CkNN is the unique unweighted construction that yields a geometry consistent with the connected components of the underlying manifold in the limit of large data. Thus CkNN produces a single graph that captures all topological features simultaneously, in contrast to persistent homology, which represents each homology generator at a separate scale. As applications we derive a new fast clustering algorithm and a method to identify patterns in natural images topologically. Finally, we conjecture that CkNN is topologically consistent, meaning that the homology of the Vietoris-Rips complex (implied by the graph Laplacian) converges to the homology of the underlying manifold (implied by the Laplace-de Rham operators) in the limit of large data.},
	language = {en},
	journal = {arXiv:1606.02353 [math]},
	author = {Berry, Tyrus and Sauer, Timothy},
	month = feb,
	year = {2019},
	note = {arXiv: 1606.02353},
}

@article{berry_nonparametric_2015,
	title = {Nonparametric {Uncertainty} {Quantification} for {Stochastic} {Gradient} {Flows}},
	url = {http://arxiv.org/abs/1407.6972},
	abstract = {This paper presents a nonparametric statistical modeling method for quantifying uncertainty in stochastic gradient systems with isotropic diffusion. The central idea is to apply the diffusion maps algorithm to a training data set to produce a stochastic matrix whose generator is a discrete approximation to the backward Kolmogorov operator of the underlying dynamics. The eigenvectors of this stochastic matrix, which we will refer to as the diffusion coordinates, are discrete approximations to the eigenfunctions of the Kolmogorov operator and form an orthonormal basis for functions defined on the data set. Using this basis, we consider the projection of three uncertainty quantification (UQ) problems (prediction, filtering, and response) into the diffusion coordinates. In these coordinates, the nonlinear prediction and response problems reduce to solving systems of infinite-dimensional linear ordinary differential equations. Similarly, the continuous-time nonlinear filtering problem reduces to solving a system of infinite-dimensional linear stochastic differential equations. Solving the UQ problems then reduces to solving the corresponding truncated linear systems in finitely many diffusion coordinates. By solving these systems we give a model-free algorithm for UQ on gradient flow systems with isotropic diffusion. We numerically verify these algorithms on a 1-dimensional linear gradient flow system where the analytic solutions of the UQ problems are known. We also apply the algorithm to a chaotically forced nonlinear gradient flow system which is known to be well approximated as a stochastically forced gradient flow.},
	language = {en},
	journal = {arXiv:1407.6972 [math]},
	author = {Berry, Tyrus and Harlim, John},
	month = feb,
	year = {2015},
	note = {arXiv: 1407.6972},
}

@article{dietrich_koopman_2019,
	title = {On the {Koopman} operator of algorithms},
	url = {http://arxiv.org/abs/1907.10807},
	abstract = {A systematic mathematical framework for the study of numerical algorithms would allow comparisons, facilitate conjugacy arguments, as well as enable the discovery of improved, accelerated, data-driven algorithms. Over the course of the last century, the Koopman operator has provided a mathematical framework for the study of dynamical systems, which facilitates conjugacy arguments and can provide efficient reduced descriptions. More recently, numerical approximations of the operator have made it possible to analyze dynamical systems in a completely data-driven, essentially equation-free pipeline. Discrete or continuous time numerical algorithms (integrators, nonlinear equation solvers, optimization algorithms) are themselves dynamical systems. In this paper, we use the Koopman operator framework in the data-driven study of such algorithms and discuss benefits for analysis and acceleration of numerical computation. For algorithms acting on high-dimensional spaces by quickly contracting them towards low-dimensional manifolds, we demonstrate how basis functions adapted to the data help to construct efficient reduced representations of the operator. Our illustrative examples include the gradient descent and Nesterov optimization algorithms, as well as the Newton-Raphson algorithm.},
	language = {en},
	journal = {arXiv:1907.10807 [cs, math]},
	author = {Dietrich, Felix and Thiem, Thomas N. and Kevrekidis, Ioannis G.},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.10807},
}

@book{kutz_dynamic_2016,
	title = {Dynamic mode decomposition. {Data}-{Driven} modelling of complex systems},
	isbn = {978-1-61197-450-8},
	language = {en},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kutz, J. Nathan and Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L.},
	year = {2016},
	keywords = {EDMD},
}

@article{schmid_dynamic_2010,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112010001217/type/journal_article},
	doi = {10.1017/S0022112010001217},
	abstract = {The description of coherent features of fluid flow is essential to our understanding of fluid-dynamical and transport processes. A method is introduced that is able to extract dynamic information from flow fields that are either generated by a (direct) numerical simulation or visualized/measured in a physical experiment. The extracted dynamic modes, which can be interpreted as a generalization of global stability modes, can be used to describe the underlying physical mechanisms captured in the data sequence or to project large-scale problems onto a dynamical system of significantly fewer degrees of freedom. The concentration on subdomains of the flow field where relevant dynamics is expected allows the dissection of a complex flow into regions of localized instability phenomena and further illustrates the flexibility of the method, as does the description of the dynamics within a spatial framework. Demonstrations of the method are presented consisting of a plane channel flow, flow over a two-dimensional cavity, wake flow behind a flexible membrane and a jet passing between two cylinders.},
	language = {en},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	month = aug,
	year = {2010},
	pages = {5--28},
}

@article{belkin_laplacian_2003,
	title = {Laplacian {Eigenmaps} for {Dimensionality} {Reduction} and {Data} {Representation}},
	volume = {15},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976603321780317},
	doi = {10.1162/089976603321780317},
	language = {en},
	number = {6},
	journal = {Neural Computation},
	author = {Belkin, Mikhail and Niyogi, Partha},
	month = jun,
	year = {2003},
	pages = {1373--1396},
}

@article{donoho_hessian_2003,
	title = {Hessian eigenmaps: {Locally} linear embedding techniques for high-dimensional data},
	volume = {100},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Hessian eigenmaps},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1031596100},
	doi = {10.1073/pnas.1031596100},
	language = {en},
	number = {10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Donoho, D. L. and Grimes, C.},
	month = may,
	year = {2003},
	pages = {5591--5596},
}

@incollection{takens_detecting_1981,
	address = {Berlin, Heidelberg},
	title = {Detecting strange attractors in turbulence},
	volume = {898},
	isbn = {978-3-540-11171-9 978-3-540-38945-3},
	url = {http://link.springer.com/10.1007/BFb0091924},
	language = {en},
	booktitle = {Dynamical {Systems} and {Turbulence}, {Warwick} 1980},
	publisher = {Springer Berlin Heidelberg},
	author = {Takens, Floris},
	year = {1981},
	doi = {10.1007/BFb0091924},
	note = {Series Title: Lecture Notes in Mathematics},
	pages = {366--381},
}

@book{bishop_pattern_2006,
	address = {New York},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	journal = {MACHINE LEARNING IN PYTHON},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	year = {2011},
	pages = {6},
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1517384113},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	language = {en},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pages = {3932--3937},
}

@article{giannakis_dynamics-adapted_2015,
	title = {Dynamics-{Adapted} {Cone} {Kernels}},
	volume = {14},
	issn = {1536-0040},
	url = {http://epubs.siam.org/doi/10.1137/140954544},
	doi = {10.1137/140954544},
	abstract = {We present a family of kernels for analysis of data generated by dynamical systems. These so-called cone kernels feature a dependence on the dynamical vector field operating in the phase space manifold, estimated empirically through finite differences of time-ordered data samples. In particular, cone kernels assign strong affinity to pairs of samples whose relative displacement vector lies within a narrow cone aligned with the dynamical vector field. The outcome of this explicit dependence on the dynamics is that, in a suitable asymptotic limit, Laplace{\textendash}Beltrami operators for data analysis constructed from cone kernels generate diffusions along the integral curves of the dynamical vector field. This property is independent of the observation modality, and endows these operators with invariance under a weakly restrictive class of transformations of the data (including conformal transformations), while it also enhances their capability to extract intrinsic dynamical timescales via eigenfunctions. Here, we study these features by establishing the Riemannian metric tensor induced by cone kernels in the limit of large data. We find that the corresponding Dirichlet energy is governed by the directional derivative of functions along the dynamical vector field, giving rise to a measure of roughness of functions that favors slowly varying observables. We demonstrate the utility of cone kernels in nonlinear flows on the 2-torus and North Pacific sea surface temperature data generated by a comprehensive climate model.},
	language = {en},
	number = {2},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Giannakis, Dimitrios},
	month = jan,
	year = {2015},
	pages = {556--608},
}

@article{klus_data-driven_2020,
	title = {Data-driven approximation of the {Koopman} generator: {Model} reduction, system identification, and control},
	volume = {406},
	issn = {01672789},
	shorttitle = {Data-driven approximation of the {Koopman} generator},
	url = {http://arxiv.org/abs/1909.10638},
	doi = {10.1016/j.physd.2020.132416},
	abstract = {We derive a data-driven method for the approximation of the Koopman generator called gEDMD, which can be regarded as a straightforward extension of EDMD (extended dynamic mode decomposition). This approach is applicable to deterministic and stochastic dynamical systems. It can be used for computing eigenvalues, eigenfunctions, and modes of the generator and for system identification. In addition to learning the governing equations of deterministic systems, which then reduces to SINDy (sparse identification of nonlinear dynamics), it is possible to identify the drift and diffusion terms of stochastic differential equations from data. Moreover, we apply gEDMD to derive coarse-grained models of high-dimensional systems, and also to determine efficient model predictive control strategies. We highlight relationships with other methods and demonstrate the efficacy of the proposed methods using several guiding examples and prototypical molecular dynamics problems.},
	language = {en},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Klus, Stefan and N{\"u}ske, Feliks and Peitz, Sebastian and Niemann, Jan-Hendrik and Clementi, Cecilia and Sch{\"u}tte, Christof},
	month = may,
	year = {2020},
	note = {arXiv: 1909.10638},
	keywords = {EDMD},
	pages = {132416},
}

@article{demo_pydmd_2018,
	title = {{PyDMD}: {Python} {Dynamic} {Mode} {Decomposition}},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {{PyDMD}},
	url = {http://joss.theoj.org/papers/10.21105/joss.00530},
	doi = {10.21105/joss.00530},
	abstract = {Dynamic mode decomposition (DMD) is a model reduction algorithm developed by Schmid (Schmid 2010). Since then has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. It is used for a data-driven model simplification based on spatiotemporal coherent structures. DMD relies only on the high-fidelity measurements, like experimental data and numerical simulations, so it is an equation-free algorithm. Its popularity is also due to the fact that it does not make any assumptions about the underlying system. See (Kutz et al. 2016) for a comprehensive overview of the algorithm and its connections to the Koopman-operator analysis, initiated in (Koopman 1931), along with examples in computational fluid dynamics. In the last years many variants arose, such as multiresolution DMD, compressed DMD, forward backward DMD, and higher order DMD among others, in order to deal with noisy data, big dataset, or spurius data for example. In the PyDMD package ({\textquotedblleft}PyDMD: Python Dynamic Mode Decomposition. Available at: https://github.com/mathLab/PyDMD{\textquotedblright} n.d.) we implemented in Python the majority of the variants mentioned above with a user friendly interface. We also provide many tutorials that show all the characteristics of the software, ranging from the basic use case to the most sofisticated one allowed by the package.},
	language = {en},
	number = {22},
	journal = {The Journal of Open Source Software},
	author = {Demo, Nicola and Tezzele, Marco and Rozza, Gianluigi},
	month = feb,
	year = {2018},
	pages = {530},
}

@article{rowley_spectral_2009,
	title = {Spectral analysis of nonlinear flows},
	volume = {641},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112009992059/type/journal_article},
	doi = {10.1017/S0022112009992059},
	abstract = {We present a technique for describing the global behaviour of complex nonlinear flows by decomposing the flow into modes determined from spectral analysis of the Koopman operator, an infinite-dimensional linear operator associated with the full nonlinear system. These modes, referred to as Koopman modes, are associated with a particular observable, and may be determined directly from data (either numerical or experimental) using a variant of a standard Arnoldi method. They have an associated temporal frequency and growth rate and may be viewed as a nonlinear generalization of global eigenmodes of a linearized system. They provide an alternative to proper orthogonal decomposition, and in the case of periodic data the Koopman modes reduce to a discrete temporal Fourier transform. The Arnoldi method used for computations is identical to the dynamic mode decomposition recently proposed by Schmid \& Sesterhenn (
              Sixty-First Annual Meeting of the APS Division of Fluid Dynamics
              , 2008), so dynamic mode decomposition can be thought of as an algorithm for finding Koopman modes. We illustrate the method on an example of a jet in crossflow, and show that the method captures the dominant frequencies and elucidates the associated spatial structures.},
	language = {en},
	journal = {Journal of Fluid Mechanics},
	author = {Rowley, Clarence W. and Mezi{\'c}, Igor and Bagheri, Shervin and Schlatter, Philipp and Henningson, Dan S.},
	month = dec,
	year = {2009},
	pages = {115--127},
}

@article{manojlovic_applications_2020,
	title = {Applications of {Koopman} {Mode} {Analysis} to {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.11765},
	abstract = {We consider the training process of a neural network as a dynamical system acting on the high-dimensional weight space. Each epoch is an application of the map induced by the optimization algorithm and the loss function. Using this induced map, we can apply observables on the weight space and measure their evolution. The evolution of the observables are given by the Koopman operator associated with the induced dynamical system. We use the spectrum and modes of the Koopman operator to realize the above objectives. Our methods can help to, a priori, determine the network depth; determine if we have a bad initialization of the network weights, allowing a restart before training too long; speeding up the training time. Additionally, our methods help enable noise rejection and improve robustness. We show how the Koopman spectrum can be used to determine the number of layers required for the architecture. Additionally, we show how we can elucidate the convergence versus non-convergence of the training process by monitoring the spectrum, in particular, how the existence of eigenvalues clustering around 1 determines when to terminate the learning process. We also show how using Koopman modes we can selectively prune the network to speed up the training procedure. Finally, we show that incorporating loss functions based on negative Sobolev norms can allow for the reconstruction of a multi-scale signal polluted by very large amounts of noise.},
	language = {en},
	journal = {arXiv:2006.11765 [cs, math, stat]},
	author = {Manojlovi{\'c}, Iva and Fonoberova, Maria and Mohr, Ryan and Andrej{\v c}uk, Aleksandr and Drma{\v c}, Zlatko and Kevrekidis, Yannis and Mezi{\'c}, Igor},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.11765},
}
