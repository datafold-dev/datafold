
@article{le_clainche_higher_2017,
	title = {Higher order dynamic mode decomposition of noisy experimental data: {The} flow structure of a zero-net-mass-flux jet},
	volume = {88},
	issn = {08941777},
	shorttitle = {Higher order dynamic mode decomposition of noisy experimental data},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089417771730184X},
	doi = {10.1016/j.expthermflusci.2017.06.011},
	abstract = {A method is presented to treat complex experimental flow data resulting from PIV. The method is based on an appropriate combination of higher order singular value decomposition (which cleans the data along the temporal dimension and the various space dimensions) and higher order dynamic mode decomposition (HODMD), a recent extension of standard dynamic mode decomposition that treats the data in a sliding window. The performance of the method is tested using experimental data obtained in the near field of a zero-net-mass-flux (ZNMF) jet. The better performance of HODMD is put in evidence making this technique suitable to both, cleaning the experimental noise using a limited number of snapshots and obtaining robust and sufficiently accurate results that elucidate the spatio-temporal structure of the flow. The results show that this ZNMF jet is temporally periodic in the near field, where the flow results from the interaction of a large number harmonics. These harmonics involve large scale spatial flow structures, identified as spatially growing instabilities, which are associated with the flow transition to turbulence in the far field.},
	language = {en},
	urldate = {2019-03-06},
	journal = {Experimental Thermal and Fluid Science},
	author = {Le Clainche, Soledad and Vega, Jos{\'e} M. and Soria, Julio},
	month = nov,
	year = {2017},
	pages = {336--353},
}

@article{berry_variable_2016,
	title = {Variable bandwidth diffusion kernels},
	volume = {40},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520315000020},
	doi = {10.1016/j.acha.2015.01.001},
	abstract = {Practical applications of kernel methods often use variable bandwidth kernels, also known as self-tuning kernels, however much of the current theory of kernel based techniques is only applicable to fixed bandwidth kernels. In this paper, we derive the asymptotic expansion of these variable bandwidth kernels for arbitrary bandwidth functions; generalizing the theory of Diffusion Maps and Laplacian Eigenmaps. We also derive pointwise error estimates for the corresponding discrete operators which are based on finite data sets; generalizing a result of Singer which was restricted to fixed bandwidth kernels. Our analysis reveals how areas of small sampling density lead to large errors, particularly for fixed bandwidth kernels. We explain the limitation of the existing theory to data sampled from compact manifolds by showing that when the sampling density is not bounded away from zero (which implies that the data lies on an open set) the error estimates for fixed bandwidth kernels will be unbounded. We show that this limitation can be overcome by choosing a bandwidth function inversely proportional to the sampling density (which can be estimated from data) which allows us to control the error estimates uniformly over a noncompact manifold. We numerically verify these results on non-compact manifolds by constructing the generator of the Ornstein{\textendash}Uhlenbeck process on a real line and a two-dimensional plane using data sampled independently from the respective invariant measures. We also verify our results on compact manifolds by constructing the Laplacian on the unit circle and the unit sphere and we show that the variable bandwidth kernels exhibit reduced sensitivity to bandwidth selection and give better results for an automatic bandwidth selection algorithm.},
	language = {en},
	number = {1},
	urldate = {2019-03-06},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Berry, Tyrus and Harlim, John},
	month = jan,
	year = {2016},
	pages = {68--96},
}

@article{dsilva_parsimonious_2018,
	title = {Parsimonious representation of nonlinear dynamical systems through manifold learning: {A} chemotaxis case study},
	volume = {44},
	issn = {10635203},
	shorttitle = {Parsimonious representation of nonlinear dynamical systems through manifold learning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520315000949},
	doi = {10.1016/j.acha.2015.06.008},
	abstract = {Nonlinear manifold learning algorithms, such as diffusion maps, have been fruitfully applied in recent years to the analysis of large and complex data sets. However, such algorithms still encounter challenges when faced with real data. One such challenge is the existence of {\textquotedblleft}repeated eigendirections,{\textquotedblright} which obscures the detection of the true dimensionality of the underlying manifold and arises when several embedding coordinates parametrize the same direction in the intrinsic geometry of the data set. We propose an algorithm, based on local linear regression, to automatically detect coordinates corresponding to repeated eigendirections. We construct a more parsimonious embedding using only the eigenvectors corresponding to unique eigendirections, and we show that this reduced diffusion maps embedding induces a metric which is equivalent to the standard diffusion distance. We first demonstrate the utility and flexibility of our approach on synthetic data sets. We then apply our algorithm to data collected from a stochastic model of cellular chemotaxis, where our approach for factoring out repeated eigendirections allows us to detect changes in dynamical behavior and the underlying intrinsic system dimensionality directly from data.},
	language = {en},
	number = {3},
	urldate = {2019-03-06},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Dsilva, Carmeline J. and Talmon, Ronen and Coifman, Ronald R. and Kevrekidis, Ioannis G.},
	month = may,
	year = {2018},
	pages = {759--773},
}

@article{tu_dynamic_2014,
	title = {On {Dynamic} {Mode} {Decomposition}: {Theory} and {Applications}},
	volume = {1},
	issn = {2158-2491},
	shorttitle = {On {Dynamic} {Mode} {Decomposition}},
	url = {http://arxiv.org/abs/1312.0041},
	doi = {10.3934/jcd.2014.1.391},
	abstract = {Originally introduced in the fluid mechanics community, dynamic mode decomposition (DMD) has emerged as a powerful tool for analyzing the dynamics of nonlinear systems. However, existing DMD theory deals primarily with sequential time series for which the measurement dimension is much larger than the number of measurements taken. We present a theoretical framework in which we define DMD as the eigendecomposition of an approximating linear operator. This generalizes DMD to a larger class of datasets, including nonsequential time series. We demonstrate the utility of this approach by presenting novel sampling strategies that increase computational efficiency and mitigate the effects of noise, respectively. We also introduce the concept of linear consistency, which helps explain the potential pitfalls of applying DMD to rank-deficient datasets, illustrating with examples. Such computations are not considered in the existing literature, but can be understood using our more general framework. In addition, we show that our theory strengthens the connections between DMD and Koopman operator theory. It also establishes connections between DMD and other techniques, including the eigensystem realization algorithm (ERA), a system identification method, and linear inverse modeling (LIM), a method from climate science. We show that under certain conditions, DMD is equivalent to LIM.},
	language = {en},
	number = {2},
	urldate = {2019-03-06},
	journal = {Journal of Computational Dynamics},
	author = {Tu, Jonathan H. and Rowley, Clarence W. and Luchtenburg, Dirk M. and Brunton, Steven L. and Kutz, J. Nathan},
	month = dec,
	year = {2014},
	note = {arXiv: 1312.0041},
	keywords = {koopman},
	pages = {391--421},
}

@article{williams_datadriven_2015,
	title = {A {Data}{\textendash}{Driven} {Approximation} of the {Koopman} {Operator}: {Extending} {Dynamic} {Mode} {Decomposition}},
	volume = {25},
	issn = {0938-8974, 1432-1467},
	shorttitle = {A {Data}{\textendash}{Driven} {Approximation} of the {Koopman} {Operator}},
	url = {http://link.springer.com/10.1007/s00332-015-9258-5},
	doi = {10.1007/s00332-015-9258-5},
	language = {en},
	number = {6},
	urldate = {2019-03-13},
	journal = {Journal of Nonlinear Science},
	author = {Williams, Matthew O. and Kevrekidis, Ioannis G. and Rowley, Clarence W.},
	month = dec,
	year = {2015},
	pages = {1307--1346},
}

@phdthesis{lafon_diffusion_2004,
	title = {Diffusion {Maps} and {Geometric} {Harmonics}},
	language = {en},
	school = {Yale University},
	author = {Lafon, St{\'e}phane S},
	year = {2004},
}

@inproceedings{rabin_heterogeneous_2012,
	title = {Heterogeneous datasets representation and learning using diffusion maps and {Laplacian} pyramids},
	isbn = {978-1-61197-232-0 978-1-61197-282-5},
	url = {https://epubs.siam.org/doi/10.1137/1.9781611972825.17},
	doi = {10.1137/1.9781611972825.17},
	abstract = {The diffusion maps together with the geometric harmonics provide a method for describing the geometry of high dimensional data and for extending these descriptions to new data points and to functions, which are defined on the data. This method suffers from two limitations. First, even though real-life data is often heterogeneous , the assumption in diffusion maps is that the attributes of the processed dataset are comparable. Second, application of the geometric harmonics requires careful setting for the correct extension scale and condition number. In this paper, we propose a method for representing and learning heterogeneous datasets by using diffusion maps for unifying and embedding heterogeneous dataset and by replacing the geometric harmonics with the Laplacian pyramid extension. Experimental results on three benchmark datasets demonstrate how the learning process becomes straightforward when the constructed representation smoothly parameterizes the task-related function.},
	language = {en},
	urldate = {2019-12-14},
	booktitle = {Proceedings of the 2012 {SIAM} {International} {Conference} on {Data} {Mining}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Rabin, Neta and Coifman, Ronald R.},
	month = apr,
	year = {2012},
	pages = {189--199},
}

@article{coifman_geometric_2006,
	title = {Geometric harmonics: {A} novel tool for multiscale out-of-sample extension of empirical functions},
	volume = {21},
	issn = {10635203},
	shorttitle = {Geometric harmonics},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000522},
	doi = {10.1016/j.acha.2005.07.005},
	abstract = {We describe a simple scheme, based on the Nystr{\"o}m method, for extending empirical functions f defined on a set X to a larger set X{\textasciimacron} . The extension process that we describe involves the construction of a specific family of functions that we term geometric harmonics. These functions constitute a generalization of the prolate spheroidal wave functions of Slepian in the sense that they are optimally concentrated on X. We study the case when X is a submanifold of Rn in greater detail. In this situation, any empirical function f on X can be characterized by its decomposition over the intrinsic Fourier modes, i.e., the eigenfunctions of the Laplace{\textendash}Beltrami operator, and we show that this intrinsic frequency spectrum determines the largest domain of extension of f to the entire space Rn. Our analysis relates the complexity of the function on the training set to the scale of extension off this set. This approach allows us to present a novel multiscale extension scheme for empirical functions.},
	language = {en},
	number = {1},
	urldate = {2019-12-15},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
	month = jul,
	year = {2006},
	pages = {31--52},
}

@article{coifman_diffusion_2006,
	title = {Diffusion maps},
	volume = {21},
	issn = {10635203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1063520306000546},
	doi = {10.1016/j.acha.2006.04.006},
	abstract = {In this paper, we provide a framework based upon diffusion processes for finding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efficient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, defines multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it unifies ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.},
	language = {en},
	number = {1},
	urldate = {2019-12-15},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Coifman, Ronald R. and Lafon, St{\'e}phane},
	month = jul,
	year = {2006},
	pages = {5--30},
}

@article{fernandez_auto-adaptative_2014,
	title = {Auto-adaptative {Laplacian} {Pyramids} for {High}-dimensional {Data} {Analysis}},
	url = {http://arxiv.org/abs/1311.6594},
	abstract = {Non-linear dimensionality reduction techniques such as manifold learning algorithms have become a common way for processing and analyzing high-dimensional patterns that often have attached a target that corresponds to the value of an unknown function. Their application to new points consists in two steps: first, embedding the new data point into the low dimensional space and then, estimating the function value on the test point from its neighbors in the embedded space.},
	language = {en},
	urldate = {2019-12-22},
	journal = {arXiv:1311.6594 [cs, stat]},
	author = {Fern{\'a}ndez, {\'A}ngela and Rabin, Neta and Fishelov, Dalia and Dorronsoro, Jos{\'e} R.},
	month = may,
	year = {2014},
	note = {arXiv: 1311.6594},
}

@article{berry_nonparametric_2015,
	title = {Nonparametric {Uncertainty} {Quantification} for {Stochastic} {Gradient} {Flows}},
	url = {http://arxiv.org/abs/1407.6972},
	abstract = {This paper presents a nonparametric statistical modeling method for quantifying uncertainty in stochastic gradient systems with isotropic diffusion. The central idea is to apply the diffusion maps algorithm to a training data set to produce a stochastic matrix whose generator is a discrete approximation to the backward Kolmogorov operator of the underlying dynamics. The eigenvectors of this stochastic matrix, which we will refer to as the diffusion coordinates, are discrete approximations to the eigenfunctions of the Kolmogorov operator and form an orthonormal basis for functions defined on the data set. Using this basis, we consider the projection of three uncertainty quantification (UQ) problems (prediction, filtering, and response) into the diffusion coordinates. In these coordinates, the nonlinear prediction and response problems reduce to solving systems of infinite-dimensional linear ordinary differential equations. Similarly, the continuous-time nonlinear filtering problem reduces to solving a system of infinite-dimensional linear stochastic differential equations. Solving the UQ problems then reduces to solving the corresponding truncated linear systems in finitely many diffusion coordinates. By solving these systems we give a model-free algorithm for UQ on gradient flow systems with isotropic diffusion. We numerically verify these algorithms on a 1-dimensional linear gradient flow system where the analytic solutions of the UQ problems are known. We also apply the algorithm to a chaotically forced nonlinear gradient flow system which is known to be well approximated as a stochastically forced gradient flow.},
	language = {en},
	urldate = {2020-01-06},
	journal = {arXiv:1407.6972 [math]},
	author = {Berry, Tyrus and Harlim, John},
	month = feb,
	year = {2015},
	note = {arXiv: 1407.6972},
}

@book{kutz_dynamic_2016,
	title = {Dynamic mode decomposition. {Data}-{Driven} modelling of complex systems},
	isbn = {978-1-61197-450-8},
	language = {en},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Kutz, J. Nathan and Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L.},
	year = {2016},
}

@article{schmid_dynamic_2010,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112010001217/type/journal_article},
	doi = {10.1017/S0022112010001217},
	abstract = {The description of coherent features of fluid flow is essential to our understanding of fluid-dynamical and transport processes. A method is introduced that is able to extract dynamic information from flow fields that are either generated by a (direct) numerical simulation or visualized/measured in a physical experiment. The extracted dynamic modes, which can be interpreted as a generalization of global stability modes, can be used to describe the underlying physical mechanisms captured in the data sequence or to project large-scale problems onto a dynamical system of significantly fewer degrees of freedom. The concentration on subdomains of the flow field where relevant dynamics is expected allows the dissection of a complex flow into regions of localized instability phenomena and further illustrates the flexibility of the method, as does the description of the dynamics within a spatial framework. Demonstrations of the method are presented consisting of a plane channel flow, flow over a two-dimensional cavity, wake flow behind a flexible membrane and a jet passing between two cylinders.},
	language = {en},
	urldate = {2020-04-09},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	month = aug,
	year = {2010},
	pages = {5--28},
}

@incollection{rand_detecting_1981,
	address = {Berlin, Heidelberg},
	title = {Detecting strange attractors in turbulence},
	volume = {898},
	isbn = {978-3-540-11171-9 978-3-540-38945-3},
	url = {http://link.springer.com/10.1007/BFb0091924},
	language = {en},
	urldate = {2020-04-29},
	booktitle = {Dynamical {Systems} and {Turbulence}, {Warwick} 1980},
	publisher = {Springer Berlin Heidelberg},
	author = {Takens, Floris},
	editor = {Rand, David and Young, Lai-Sang},
	year = {1981},
	doi = {10.1007/BFb0091924},
	note = {Series Title: Lecture Notes in Mathematics},
	pages = {366--381},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	journal = {MACHINE LEARNING IN PYTHON},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	year = {2011},
	pages = {6},
}